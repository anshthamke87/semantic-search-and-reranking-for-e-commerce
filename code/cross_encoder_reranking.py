# -*- coding: utf-8 -*-
"""cross_encoder_reranking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmAMIap5A1apAIGOHZbaIu3FZ_DUaGQe
"""

# =============================================================================
# NOTEBOOK 4: CROSS-ENCODER RERANKING
# Runtime: GPU (T4) - Keep GPU runtime for cross-encoder inference
# Estimated Time: 30-45 minutes
# =============================================================================

# =============================================================================
# SECTION 1: PACKAGE INSTALLATION
# =============================================================================

# Install required packages for cross-encoder reranking
!pip install sentence-transformers -q
!pip install ir_measures -q
!pip install pyyaml -q

print("‚úÖ Packages installed successfully!")

# =============================================================================
# SECTION 2: IMPORTS AND SETUP
# =============================================================================

import pandas as pd
import numpy as np
import json
import os
import time
import gc
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import yaml

# Sentence Transformers for cross-encoder
from sentence_transformers import CrossEncoder

# IR evaluation
import ir_measures
from ir_measures import *

# PyTorch utilities
import torch
print(f"üî• GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üéØ GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

PROJECT_DIR = '/content/drive/MyDrive/semantic-search'
print(f"‚úÖ Project directory: {PROJECT_DIR}")

# =============================================================================
# SECTION 3: LOAD DATASET AND PREVIOUS RESULTS
# =============================================================================

print("üìÇ Loading dataset and previous results...")

# Load dataset
queries_df = pd.read_csv(f"{PROJECT_DIR}/data/queries.csv")
corpus_df = pd.read_csv(f"{PROJECT_DIR}/data/corpus.csv")
qrels_df = pd.read_csv(f"{PROJECT_DIR}/data/qrels.csv")

print(f"‚úÖ Dataset loaded:")
print(f"   üìã Queries: {len(queries_df)}")
print(f"   üì¶ Products: {len(corpus_df)}")
print(f"   üéØ Qrels: {len(qrels_df)}")

# Load previous baseline results for comparison
try:
    with open(f"{PROJECT_DIR}/reports/bm25_baseline_metrics.json", 'r') as f:
        bm25_baseline = json.load(f)
    print(f"üìä BM25 Baseline nDCG@10: {bm25_baseline['metrics'].get('nDCG@10', 0):.4f}")
except:
    bm25_baseline = {'metrics': {'nDCG@10': 0.0025}}

try:
    with open(f"{PROJECT_DIR}/reports/semantic_retrieval_metrics.json", 'r') as f:
        semantic_baseline = json.load(f)
    print(f"üìä Semantic Baseline nDCG@10: {semantic_baseline['metrics'].get('nDCG@10', 0):.4f}")
except:
    semantic_baseline = {'metrics': {'nDCG@10': 0.0031}}

# Load semantic retrieval results (our candidates for reranking)
semantic_results_df = pd.read_csv(f"{PROJECT_DIR}/runs/bi_encoder_results.csv")
print(f"üìä Semantic results loaded: {len(semantic_results_df)} candidate pairs")

# Load configuration
with open(f"{PROJECT_DIR}/cfg/retrieval.yaml", 'r') as f:
    config = yaml.safe_load(f)

cross_encoder_config = config['cross_encoder']
print(f"üéØ Cross-encoder config: {cross_encoder_config}")

# =============================================================================
# SECTION 4: LOAD CROSS-ENCODER MODEL
# =============================================================================

print("üéØ Loading cross-encoder model...")

model_name = cross_encoder_config['model_name']
print(f"üì• Loading cross-encoder: {model_name}")

# Load cross-encoder model
cross_encoder = CrossEncoder(model_name)

# Move to GPU if available
if torch.cuda.is_available():
    cross_encoder.model = cross_encoder.model.to('cuda')
    print("üî• Cross-encoder moved to GPU")

print(f"‚úÖ Cross-encoder loaded successfully")

# Test cross-encoder with sample query-document pair
sample_query = "apple wireless headphones"
sample_doc = "Apple Wireless Black Headphones. High quality headphones from Apple. Perfect for daily use."
sample_score = cross_encoder.predict([(sample_query, sample_doc)])

print(f"\nüß™ Cross-encoder test:")
print(f"   Query: '{sample_query}'")
print(f"   Document: '{sample_doc[:60]}...'")
print(f"   Relevance score: {sample_score[0]:.4f}")

# =============================================================================
# SECTION 5: PREPARE CANDIDATES FOR RERANKING
# =============================================================================

print("üìä Preparing candidates for cross-encoder reranking...")

# Group semantic results by query
semantic_by_query = semantic_results_df.groupby('query_id')

# Get top-k candidates per query for reranking
top_k_for_reranking = cross_encoder_config['top_k']  # Rerank top-50
print(f"üéØ Reranking top-{top_k_for_reranking} candidates per query")

# Prepare query-document pairs for cross-encoder
rerank_candidates = []
query_candidate_counts = []

for query_id, group in tqdm(semantic_by_query, desc="Preparing candidates"):
    # Get top-k candidates from semantic retrieval
    top_candidates = group.head(top_k_for_reranking)

    query_text = queries_df[queries_df['query_id'] == query_id].iloc[0]['query']

    for _, candidate in top_candidates.iterrows():
        # Get document text
        doc_id = candidate['doc_id']
        product = corpus_df[corpus_df['product_id'] == doc_id].iloc[0]
        doc_text = f"{product['title']}. {product['description']} Brand: {product['brand']}. Category: {product['category']}."

        rerank_candidates.append({
            'query_id': query_id,
            'query_text': query_text,
            'doc_id': doc_id,
            'doc_text': doc_text,
            'semantic_score': candidate['score'],
            'semantic_rank': candidate['rank']
        })

    query_candidate_counts.append(len(top_candidates))

print(f"‚úÖ Prepared {len(rerank_candidates)} query-document pairs for reranking")
print(f"üìä Average candidates per query: {np.mean(query_candidate_counts):.1f}")

# Show sample candidates
print(f"\nüìã Sample candidates for reranking:")
for i in range(3):
    candidate = rerank_candidates[i]
    print(f"   Query: '{candidate['query_text']}'")
    print(f"   Doc: '{candidate['doc_text'][:80]}...'")
    print(f"   Semantic score: {candidate['semantic_score']:.4f}")
    print(f"   ---")

# =============================================================================
# SECTION 6: RUN CROSS-ENCODER RERANKING
# =============================================================================

print("üéØ Running cross-encoder reranking...")

batch_size = cross_encoder_config['batch_size']
print(f"üì¶ Batch size: {batch_size}")

# Prepare query-document pairs for batch processing
query_doc_pairs = [(c['query_text'], c['doc_text']) for c in rerank_candidates]

start_time = time.time()
all_ce_scores = []
processing_times = []

# Process in batches
num_batches = (len(query_doc_pairs) + batch_size - 1) // batch_size
print(f"üìä Processing {num_batches} batches...")

for i in tqdm(range(0, len(query_doc_pairs), batch_size), desc="Cross-encoder reranking"):
    batch_pairs = query_doc_pairs[i:i + batch_size]

    # Measure batch processing time
    batch_start = time.time()

    # Get cross-encoder scores for batch
    batch_scores = cross_encoder.predict(batch_pairs)

    batch_time = time.time() - batch_start
    processing_times.append(batch_time)

    all_ce_scores.extend(batch_scores)

    # Clear GPU cache periodically
    if torch.cuda.is_available() and i % (batch_size * 10) == 0:
        torch.cuda.empty_cache()

total_reranking_time = time.time() - start_time

print(f"‚úÖ Cross-encoder reranking completed in {total_reranking_time:.1f} seconds")
print(f"üìä Processed {len(all_ce_scores)} query-document pairs")
print(f"‚è±Ô∏è Average batch time: {np.mean(processing_times):.3f} seconds")

# Add cross-encoder scores to candidates
for i, candidate in enumerate(rerank_candidates):
    candidate['ce_score'] = all_ce_scores[i]

# Show score distribution
ce_scores_array = np.array(all_ce_scores)
print(f"\nüìä Cross-encoder score statistics:")
print(f"   Min: {np.min(ce_scores_array):.4f}")
print(f"   Max: {np.max(ce_scores_array):.4f}")
print(f"   Mean: {np.mean(ce_scores_array):.4f}")
print(f"   Std: {np.std(ce_scores_array):.4f}")

# =============================================================================
# SECTION 7: RERANK AND GENERATE FINAL RESULTS
# =============================================================================

print("üîÑ Reranking candidates and generating final results...")

# Group candidates by query and rerank by cross-encoder score
final_results = []
reranking_stats = []

for query_id in tqdm(queries_df['query_id'], desc="Final reranking"):
    # Get candidates for this query
    query_candidates = [c for c in rerank_candidates if c['query_id'] == query_id]

    if len(query_candidates) == 0:
        continue

    # Sort by cross-encoder score (descending)
    query_candidates.sort(key=lambda x: x['ce_score'], reverse=True)

    # Store reranking statistics
    semantic_scores = [c['semantic_score'] for c in query_candidates]
    ce_scores = [c['ce_score'] for c in query_candidates]

    # Calculate rank correlation between semantic and cross-encoder
    from scipy.stats import spearmanr
    try:
        correlation, _ = spearmanr(semantic_scores, ce_scores)
        reranking_stats.append(correlation)
    except:
        reranking_stats.append(0.0)

    # Generate final ranked results
    for rank, candidate in enumerate(query_candidates):
        final_results.append({
            'query_id': candidate['query_id'],
            'Q0': 'Q0',
            'doc_id': candidate['doc_id'],
            'rank': rank + 1,
            'score': candidate['ce_score'],
            'run_id': 'CrossEncoder'
        })

print(f"‚úÖ Generated {len(final_results)} final reranked results")
print(f"üìä Average semantic-CE correlation: {np.mean(reranking_stats):.3f}")

# Show sample reranked results
sample_query_id = final_results[0]['query_id']
sample_results = [r for r in final_results if r['query_id'] == sample_query_id][:5]
sample_query_text = queries_df[queries_df['query_id'] == sample_query_id].iloc[0]['query']

print(f"\nüìã Sample reranked results for: '{sample_query_text}'")
for result in sample_results:
    product = corpus_df[corpus_df['product_id'] == result['doc_id']].iloc[0]
    original_candidate = next(c for c in rerank_candidates if c['query_id'] == result['query_id'] and c['doc_id'] == result['doc_id'])
    print(f"   Rank {result['rank']}: {product['title']}")
    print(f"      CE score: {result['score']:.4f}, Semantic score: {original_candidate['semantic_score']:.4f}")

# =============================================================================
# SECTION 8: SAVE CROSS-ENCODER RESULTS
# =============================================================================

print("\nüíæ Saving cross-encoder results...")

# Create results dataframe
ce_runs_df = pd.DataFrame(final_results)

# Save in TREC format
trec_run_path = f"{PROJECT_DIR}/runs/cross_encoder_rerank.trec"
with open(trec_run_path, 'w') as f:
    for _, row in ce_runs_df.iterrows():
        f.write(f"{row['query_id']} {row['Q0']} {row['doc_id']} {row['rank']} {row['score']:.6f} {row['run_id']}\n")

print(f"‚úÖ Cross-encoder run file saved: {trec_run_path}")

# Save as CSV for analysis
ce_runs_df.to_csv(f"{PROJECT_DIR}/runs/cross_encoder_results.csv", index=False)

# Save detailed candidates with all scores
candidates_df = pd.DataFrame(rerank_candidates)
candidates_df.to_csv(f"{PROJECT_DIR}/runs/reranking_candidates.csv", index=False)

print(f"‚úÖ Results and candidates saved")

# =============================================================================
# SECTION 9: EVALUATE CROSS-ENCODER PERFORMANCE
# =============================================================================

print("\nüìä Evaluating cross-encoder performance...")

# Load qrels
qrels_path = f"{PROJECT_DIR}/data/qrels.trec"
qrels_dict = {}
with open(qrels_path, 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 4:
            qid, _, docid, rel = parts
            if qid not in qrels_dict:
                qrels_dict[qid] = {}
            qrels_dict[qid][docid] = int(rel)

# Create run dict
run_dict = {}
for _, row in ce_runs_df.iterrows():
    qid = row['query_id']
    if qid not in run_dict:
        run_dict[qid] = {}
    run_dict[qid][row['doc_id']] = float(row['score'])

print(f"üìã Evaluation setup:")
print(f"   Qrels queries: {len(qrels_dict)}")
print(f"   Run queries: {len(run_dict)}")

# Calculate metrics
metrics = [nDCG@10, nDCG@20, MAP, MRR, P@10, Recall@100]
results = ir_measures.calc_aggregate(metrics, qrels_dict, run_dict)

print("\n" + "="*50)
print("üìà CROSS-ENCODER RERANKING RESULTS")
print("="*50)

ce_metrics = {}
for metric in results:
    value = results[metric]
    ce_metrics[str(metric)] = value
    print(f"{str(metric)}: {value:.4f}")

# Compare with previous baselines
print(f"\nüîÑ Performance Comparison:")
bm25_ndcg10 = bm25_baseline['metrics'].get('nDCG@10', 0)
semantic_ndcg10 = semantic_baseline['metrics'].get('nDCG@10', 0)
ce_ndcg10 = ce_metrics.get('nDCG@10', 0)

print(f"   üìä BM25 nDCG@10: {bm25_ndcg10:.4f}")
print(f"   üìä Semantic nDCG@10: {semantic_ndcg10:.4f}")
print(f"   üìä Cross-Encoder nDCG@10: {ce_ndcg10:.4f}")

# Calculate improvements
semantic_improvement = ((semantic_ndcg10 - bm25_ndcg10) / max(bm25_ndcg10, 0.0001)) * 100
ce_improvement = ((ce_ndcg10 - semantic_ndcg10) / max(semantic_ndcg10, 0.0001)) * 100
total_improvement = ((ce_ndcg10 - bm25_ndcg10) / max(bm25_ndcg10, 0.0001)) * 100

print(f"\nüìà Improvement Analysis:")
print(f"   BM25 ‚Üí Semantic: {semantic_improvement:+.1f}%")
print(f"   Semantic ‚Üí Cross-Encoder: {ce_improvement:+.1f}%")
print(f"   BM25 ‚Üí Cross-Encoder (Total): {total_improvement:+.1f}%")

if ce_ndcg10 > semantic_ndcg10:
    print(f"   ‚úÖ Cross-encoder improves over semantic retrieval!")
else:
    print(f"   ‚ö†Ô∏è Cross-encoder needs tuning")

# =============================================================================
# SECTION 10: TIMING AND EFFICIENCY ANALYSIS
# =============================================================================

print("\n" + "="*50)
print("‚è±Ô∏è CROSS-ENCODER TIMING ANALYSIS")
print("="*50)

# Calculate per-query timing
avg_pairs_per_query = len(rerank_candidates) / len(queries_df)
avg_time_per_query = total_reranking_time / len(queries_df)
avg_time_per_pair = total_reranking_time / len(rerank_candidates)

print(f"Reranking performance:")
print(f"  Total reranking time: {total_reranking_time:.1f} seconds")
print(f"  Average pairs per query: {avg_pairs_per_query:.1f}")
print(f"  Average time per query: {avg_time_per_query:.3f} seconds")
print(f"  Average time per pair: {avg_time_per_pair:.4f} seconds")

# Throughput calculations
pairs_per_second = len(rerank_candidates) / total_reranking_time
queries_per_second = len(queries_df) / total_reranking_time

print(f"  Throughput: {pairs_per_second:.1f} pairs/second")
print(f"  Query throughput: {queries_per_second:.1f} queries/second")

print(f"\nüìä Pipeline Efficiency:")
print(f"  Candidates for reranking: {len(rerank_candidates):,}")
print(f"  Average reranking depth: {top_k_for_reranking}")
print(f"  Score correlation: {np.mean(reranking_stats):.3f}")

# =============================================================================
# SECTION 11: COMPREHENSIVE VISUALIZATIONS
# =============================================================================

print("\nüìä Creating comprehensive visualizations...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Performance comparison across all methods
methods = ['BM25', 'Semantic', 'Cross-Encoder']
ndcg10_scores = [bm25_ndcg10, semantic_ndcg10, ce_ndcg10]
colors = ['blue', 'green', 'red']

axes[0,0].bar(methods, ndcg10_scores, color=colors, alpha=0.7, edgecolor='black')
axes[0,0].set_title('nDCG@10 Performance Comparison')
axes[0,0].set_ylabel('nDCG@10 Score')
axes[0,0].set_ylim(0, max(ndcg10_scores) * 1.2)
for i, score in enumerate(ndcg10_scores):
    axes[0,0].text(i, score + max(ndcg10_scores) * 0.02, f'{score:.4f}', ha='center')

# Cross-encoder score distribution
axes[0,1].hist(ce_scores_array, bins=50, alpha=0.7, edgecolor='black', color='red')
axes[0,1].set_title('Cross-Encoder Score Distribution')
axes[0,1].set_xlabel('Cross-Encoder Score')
axes[0,1].set_ylabel('Frequency')

# Semantic vs Cross-encoder score scatter plot
semantic_scores_for_plot = [c['semantic_score'] for c in rerank_candidates]
ce_scores_for_plot = [c['ce_score'] for c in rerank_candidates]
axes[0,2].scatter(semantic_scores_for_plot, ce_scores_for_plot, alpha=0.5, s=1)
axes[0,2].set_title('Semantic vs Cross-Encoder Scores')
axes[0,2].set_xlabel('Semantic Score')
axes[0,2].set_ylabel('Cross-Encoder Score')

# Correlation distribution
axes[1,0].hist(reranking_stats, bins=30, alpha=0.7, edgecolor='black', color='purple')
axes[1,0].set_title('Semantic-CE Score Correlation Distribution')
axes[1,0].set_xlabel('Spearman Correlation')
axes[1,0].set_ylabel('Frequency')
axes[1,0].axvline(np.mean(reranking_stats), color='red', linestyle='--', label=f'Mean: {np.mean(reranking_stats):.3f}')
axes[1,0].legend()

# Performance improvement breakdown
improvement_stages = ['BM25‚ÜíSemantic', 'Semantic‚ÜíCE', 'BM25‚ÜíCE (Total)']
improvement_values = [semantic_improvement, ce_improvement, total_improvement]
colors_imp = ['green', 'red', 'purple']

axes[1,1].bar(improvement_stages, improvement_values, color=colors_imp, alpha=0.7, edgecolor='black')
axes[1,1].set_title('Performance Improvement Breakdown')
axes[1,1].set_ylabel('Improvement (%)')
axes[1,1].tick_params(axis='x', rotation=45)
for i, val in enumerate(improvement_values):
    axes[1,1].text(i, val + max(improvement_values) * 0.02, f'{val:+.1f}%', ha='center')

# Timing comparison (estimated)
timing_methods = ['BM25\n(~26ms)', 'Semantic\n(~7ms)', 'Cross-Encoder\n(~{}ms)'.format(int(avg_time_per_query * 1000))]
timing_values = [26, 7, avg_time_per_query * 1000]

axes[1,2].bar(timing_methods, timing_values, color=colors, alpha=0.7, edgecolor='black')
axes[1,2].set_title('Average Query Latency Comparison')
axes[1,2].set_ylabel('Latency (ms)')

plt.tight_layout()
plt.savefig(f"{PROJECT_DIR}/reports/cross_encoder_analysis.png", dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# SECTION 12: SAVE COMPREHENSIVE REPORT
# =============================================================================

print("\nüíæ Saving comprehensive cross-encoder report...")

cross_encoder_report = {
    'model': 'CrossEncoder_Reranking',
    'cross_encoder_model': model_name,
    'dataset': {
        'total_documents': len(corpus_df),
        'total_queries': len(queries_df),
        'reranking_candidates': len(rerank_candidates)
    },
    'performance': {
        'total_results': len(final_results),
        'queries_with_results': len(set([r['query_id'] for r in final_results])),
        'avg_candidates_per_query': avg_pairs_per_query,
        'reranking_depth': top_k_for_reranking
    },
    'metrics': ce_metrics,
    'timing': {
        'total_reranking_sec': total_reranking_time,
        'avg_time_per_query_sec': avg_time_per_query,
        'avg_time_per_pair_sec': avg_time_per_pair,
        'throughput_pairs_per_sec': pairs_per_second,
        'throughput_queries_per_sec': queries_per_second
    },
    'comparison': {
        'bm25_ndcg10': bm25_ndcg10,
        'semantic_ndcg10': semantic_ndcg10,
        'cross_encoder_ndcg10': ce_ndcg10,
        'semantic_improvement_percent': semantic_improvement,
        'ce_improvement_percent': ce_improvement,
        'total_improvement_percent': total_improvement
    },
    'reranking_analysis': {
        'avg_semantic_ce_correlation': np.mean(reranking_stats),
        'ce_score_statistics': {
            'min': float(np.min(ce_scores_array)),
            'max': float(np.max(ce_scores_array)),
            'mean': float(np.mean(ce_scores_array)),
            'std': float(np.std(ce_scores_array))
        }
    },
    'config': cross_encoder_config
}

with open(f"{PROJECT_DIR}/reports/cross_encoder_metrics.json", 'w') as f:
    json.dump(cross_encoder_report, f, indent=2)

print(f"‚úÖ Comprehensive report saved")

# =============================================================================
# SECTION 13: FINAL SUMMARY AND NEXT STEPS
# =============================================================================

# Memory cleanup
del rerank_candidates, all_ce_scores
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()

print("\n" + "="*60)
print("‚úÖ CROSS-ENCODER RERANKING COMPLETED SUCCESSFULLY!")
print("="*60)

print(f"üéØ Final Performance Summary:")
print(f"   üìä nDCG@10: {ce_metrics.get('nDCG@10', 0):.4f}")
print(f"   üìä nDCG@20: {ce_metrics.get('nDCG@20', 0):.4f}")
print(f"   üìä MAP: {ce_metrics.get('MAP', 0):.4f}")
print(f"   üìä MRR: {ce_metrics.get('MRR', 0):.4f}")
print(f"   üìä P@10: {ce_metrics.get('P@10', 0):.4f}")

print(f"\nüèÜ Complete Pipeline Performance:")
print(f"   ü•â BM25 Baseline: {bm25_ndcg10:.4f}")
print(f"   ü•à + Semantic Retrieval: {semantic_ndcg10:.4f} ({semantic_improvement:+.1f}%)")
print(f"   ü•á + Cross-Encoder Reranking: {ce_ndcg10:.4f} ({ce_improvement:+.1f}%)")
print(f"   üöÄ Total Improvement: {total_improvement:+.1f}%")

print(f"\nüìä System Performance:")
print(f"   üéØ Reranking Coverage: {len(set([r['query_id'] for r in final_results]))/len(queries_df)*100:.1f}%")
print(f"   ‚è±Ô∏è Reranking Latency: {avg_time_per_query*1000:.1f} ms/query")
print(f"   üîÑ Score Correlation: {np.mean(reranking_stats):.3f}")
print(f"   üì¶ Reranking Depth: {top_k_for_reranking} candidates/query")

print(f"\nüìÅ Final Files Generated:")
print(f"   üîç TREC Run: runs/cross_encoder_rerank.trec")
print(f"   üìä Metrics: reports/cross_encoder_metrics.json")
print(f"   üìà Visualizations: reports/cross_encoder_analysis.png")
print(f"   üìã Results: runs/cross_encoder_results.csv")
print(f"   üéØ Candidates: runs/reranking_candidates.csv")

print(f"\nüéâ Three-Stage Retrieval Pipeline Complete!")
print(f"   ‚úÖ Stage 1: BM25 Baseline ‚Üí Lexical matching foundation")
print(f"   ‚úÖ Stage 2: Semantic Retrieval ‚Üí Query intent understanding")
print(f"   ‚úÖ Stage 3: Cross-Encoder Reranking ‚Üí Fine-grained relevance scoring")

print(f"\nüöÄ Ready for Phase 5: Demo & Final Report!")
print(f"   üì± Interactive demo with all three stages")
print(f"   üìä Comprehensive ablation analysis")
print(f"   üìù Technical documentation")

print("="*60)

