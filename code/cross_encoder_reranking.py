# -*- coding: utf-8 -*-
"""cross_encoder_reranking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmAMIap5A1apAIGOHZbaIu3FZ_DUaGQe
"""

# =============================================================================
# NOTEBOOK 4: CROSS-ENCODER RERANKING
# Runtime: GPU (T4) - Keep GPU runtime for cross-encoder inference
# Estimated Time: 30-45 minutes
# =============================================================================

# =============================================================================
# SECTION 1: PACKAGE INSTALLATION
# =============================================================================

# Install required packages for cross-encoder reranking
!pip install sentence-transformers -q
!pip install ir_measures -q
!pip install pyyaml -q

print("âœ… Packages installed successfully!")

# =============================================================================
# SECTION 2: IMPORTS AND SETUP
# =============================================================================

import pandas as pd
import numpy as np
import json
import os
import time
import gc
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import yaml

# Sentence Transformers for cross-encoder
from sentence_transformers import CrossEncoder

# IR evaluation
import ir_measures
from ir_measures import *

# PyTorch utilities
import torch
print(f"ğŸ”¥ GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"ğŸ¯ GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

PROJECT_DIR = '/content/drive/MyDrive/semantic-search'
print(f"âœ… Project directory: {PROJECT_DIR}")

# =============================================================================
# SECTION 3: LOAD DATASET AND PREVIOUS RESULTS
# =============================================================================

print("ğŸ“‚ Loading dataset and previous results...")

# Load dataset
queries_df = pd.read_csv(f"{PROJECT_DIR}/data/queries.csv")
corpus_df = pd.read_csv(f"{PROJECT_DIR}/data/corpus.csv")
qrels_df = pd.read_csv(f"{PROJECT_DIR}/data/qrels.csv")

print(f"âœ… Dataset loaded:")
print(f"   ğŸ“‹ Queries: {len(queries_df)}")
print(f"   ğŸ“¦ Products: {len(corpus_df)}")
print(f"   ğŸ¯ Qrels: {len(qrels_df)}")

# Load previous baseline results for comparison
try:
    with open(f"{PROJECT_DIR}/reports/bm25_baseline_metrics.json", 'r') as f:
        bm25_baseline = json.load(f)
    print(f"ğŸ“Š BM25 Baseline nDCG@10: {bm25_baseline['metrics'].get('nDCG@10', 0):.4f}")
except:
    bm25_baseline = {'metrics': {'nDCG@10': 0.0025}}

try:
    with open(f"{PROJECT_DIR}/reports/semantic_retrieval_metrics.json", 'r') as f:
        semantic_baseline = json.load(f)
    print(f"ğŸ“Š Semantic Baseline nDCG@10: {semantic_baseline['metrics'].get('nDCG@10', 0):.4f}")
except:
    semantic_baseline = {'metrics': {'nDCG@10': 0.0031}}

# Load semantic retrieval results (our candidates for reranking)
semantic_results_df = pd.read_csv(f"{PROJECT_DIR}/runs/bi_encoder_results.csv")
print(f"ğŸ“Š Semantic results loaded: {len(semantic_results_df)} candidate pairs")

# Load configuration
with open(f"{PROJECT_DIR}/cfg/retrieval.yaml", 'r') as f:
    config = yaml.safe_load(f)

cross_encoder_config = config['cross_encoder']
print(f"ğŸ¯ Cross-encoder config: {cross_encoder_config}")

# =============================================================================
# SECTION 4: LOAD CROSS-ENCODER MODEL
# =============================================================================

print("ğŸ¯ Loading cross-encoder model...")

model_name = cross_encoder_config['model_name']
print(f"ğŸ“¥ Loading cross-encoder: {model_name}")

# Load cross-encoder model
cross_encoder = CrossEncoder(model_name)

# Move to GPU if available
if torch.cuda.is_available():
    cross_encoder.model = cross_encoder.model.to('cuda')
    print("ğŸ”¥ Cross-encoder moved to GPU")

print(f"âœ… Cross-encoder loaded successfully")

# Test cross-encoder with sample query-document pair
sample_query = "apple wireless headphones"
sample_doc = "Apple Wireless Black Headphones. High quality headphones from Apple. Perfect for daily use."
sample_score = cross_encoder.predict([(sample_query, sample_doc)])

print(f"\nğŸ§ª Cross-encoder test:")
print(f"   Query: '{sample_query}'")
print(f"   Document: '{sample_doc[:60]}...'")
print(f"   Relevance score: {sample_score[0]:.4f}")

# =============================================================================
# SECTION 5: PREPARE CANDIDATES FOR RERANKING
# =============================================================================

print("ğŸ“Š Preparing candidates for cross-encoder reranking...")

# Group semantic results by query
semantic_by_query = semantic_results_df.groupby('query_id')

# Get top-k candidates per query for reranking
top_k_for_reranking = cross_encoder_config['top_k']  # Rerank top-50
print(f"ğŸ¯ Reranking top-{top_k_for_reranking} candidates per query")

# Prepare query-document pairs for cross-encoder
rerank_candidates = []
query_candidate_counts = []

for query_id, group in tqdm(semantic_by_query, desc="Preparing candidates"):
    # Get top-k candidates from semantic retrieval
    top_candidates = group.head(top_k_for_reranking)

    query_text = queries_df[queries_df['query_id'] == query_id].iloc[0]['query']

    for _, candidate in top_candidates.iterrows():
        # Get document text
        doc_id = candidate['doc_id']
        product = corpus_df[corpus_df['product_id'] == doc_id].iloc[0]
        doc_text = f"{product['title']}. {product['description']} Brand: {product['brand']}. Category: {product['category']}."

        rerank_candidates.append({
            'query_id': query_id,
            'query_text': query_text,
            'doc_id': doc_id,
            'doc_text': doc_text,
            'semantic_score': candidate['score'],
            'semantic_rank': candidate['rank']
        })

    query_candidate_counts.append(len(top_candidates))

print(f"âœ… Prepared {len(rerank_candidates)} query-document pairs for reranking")
print(f"ğŸ“Š Average candidates per query: {np.mean(query_candidate_counts):.1f}")

# Show sample candidates
print(f"\nğŸ“‹ Sample candidates for reranking:")
for i in range(3):
    candidate = rerank_candidates[i]
    print(f"   Query: '{candidate['query_text']}'")
    print(f"   Doc: '{candidate['doc_text'][:80]}...'")
    print(f"   Semantic score: {candidate['semantic_score']:.4f}")
    print(f"   ---")

# =============================================================================
# SECTION 6: RUN CROSS-ENCODER RERANKING
# =============================================================================

print("ğŸ¯ Running cross-encoder reranking...")

batch_size = cross_encoder_config['batch_size']
print(f"ğŸ“¦ Batch size: {batch_size}")

# Prepare query-document pairs for batch processing
query_doc_pairs = [(c['query_text'], c['doc_text']) for c in rerank_candidates]

start_time = time.time()
all_ce_scores = []
processing_times = []

# Process in batches
num_batches = (len(query_doc_pairs) + batch_size - 1) // batch_size
print(f"ğŸ“Š Processing {num_batches} batches...")

for i in tqdm(range(0, len(query_doc_pairs), batch_size), desc="Cross-encoder reranking"):
    batch_pairs = query_doc_pairs[i:i + batch_size]

    # Measure batch processing time
    batch_start = time.time()

    # Get cross-encoder scores for batch
    batch_scores = cross_encoder.predict(batch_pairs)

    batch_time = time.time() - batch_start
    processing_times.append(batch_time)

    all_ce_scores.extend(batch_scores)

    # Clear GPU cache periodically
    if torch.cuda.is_available() and i % (batch_size * 10) == 0:
        torch.cuda.empty_cache()

total_reranking_time = time.time() - start_time

print(f"âœ… Cross-encoder reranking completed in {total_reranking_time:.1f} seconds")
print(f"ğŸ“Š Processed {len(all_ce_scores)} query-document pairs")
print(f"â±ï¸ Average batch time: {np.mean(processing_times):.3f} seconds")

# Add cross-encoder scores to candidates
for i, candidate in enumerate(rerank_candidates):
    candidate['ce_score'] = all_ce_scores[i]

# Show score distribution
ce_scores_array = np.array(all_ce_scores)
print(f"\nğŸ“Š Cross-encoder score statistics:")
print(f"   Min: {np.min(ce_scores_array):.4f}")
print(f"   Max: {np.max(ce_scores_array):.4f}")
print(f"   Mean: {np.mean(ce_scores_array):.4f}")
print(f"   Std: {np.std(ce_scores_array):.4f}")

# =============================================================================
# SECTION 7: RERANK AND GENERATE FINAL RESULTS
# =============================================================================

print("ğŸ”„ Reranking candidates and generating final results...")

# Group candidates by query and rerank by cross-encoder score
final_results = []
reranking_stats = []

for query_id in tqdm(queries_df['query_id'], desc="Final reranking"):
    # Get candidates for this query
    query_candidates = [c for c in rerank_candidates if c['query_id'] == query_id]

    if len(query_candidates) == 0:
        continue

    # Sort by cross-encoder score (descending)
    query_candidates.sort(key=lambda x: x['ce_score'], reverse=True)

    # Store reranking statistics
    semantic_scores = [c['semantic_score'] for c in query_candidates]
    ce_scores = [c['ce_score'] for c in query_candidates]

    # Calculate rank correlation between semantic and cross-encoder
    from scipy.stats import spearmanr
    try:
        correlation, _ = spearmanr(semantic_scores, ce_scores)
        reranking_stats.append(correlation)
    except:
        reranking_stats.append(0.0)

    # Generate final ranked results
    for rank, candidate in enumerate(query_candidates):
        final_results.append({
            'query_id': candidate['query_id'],
            'Q0': 'Q0',
            'doc_id': candidate['doc_id'],
            'rank': rank + 1,
            'score': candidate['ce_score'],
            'run_id': 'CrossEncoder'
        })

print(f"âœ… Generated {len(final_results)} final reranked results")
print(f"ğŸ“Š Average semantic-CE correlation: {np.mean(reranking_stats):.3f}")

# Show sample reranked results
sample_query_id = final_results[0]['query_id']
sample_results = [r for r in final_results if r['query_id'] == sample_query_id][:5]
sample_query_text = queries_df[queries_df['query_id'] == sample_query_id].iloc[0]['query']

print(f"\nğŸ“‹ Sample reranked results for: '{sample_query_text}'")
for result in sample_results:
    product = corpus_df[corpus_df['product_id'] == result['doc_id']].iloc[0]
    original_candidate = next(c for c in rerank_candidates if c['query_id'] == result['query_id'] and c['doc_id'] == result['doc_id'])
    print(f"   Rank {result['rank']}: {product['title']}")
    print(f"      CE score: {result['score']:.4f}, Semantic score: {original_candidate['semantic_score']:.4f}")

# =============================================================================
# SECTION 8: SAVE CROSS-ENCODER RESULTS
# =============================================================================

print("\nğŸ’¾ Saving cross-encoder results...")

# Create results dataframe
ce_runs_df = pd.DataFrame(final_results)

# Save in TREC format
trec_run_path = f"{PROJECT_DIR}/runs/cross_encoder_rerank.trec"
with open(trec_run_path, 'w') as f:
    for _, row in ce_runs_df.iterrows():
        f.write(f"{row['query_id']} {row['Q0']} {row['doc_id']} {row['rank']} {row['score']:.6f} {row['run_id']}\n")

print(f"âœ… Cross-encoder run file saved: {trec_run_path}")

# Save as CSV for analysis
ce_runs_df.to_csv(f"{PROJECT_DIR}/runs/cross_encoder_results.csv", index=False)

# Save detailed candidates with all scores
candidates_df = pd.DataFrame(rerank_candidates)
candidates_df.to_csv(f"{PROJECT_DIR}/runs/reranking_candidates.csv", index=False)

print(f"âœ… Results and candidates saved")

# =============================================================================
# SECTION 9: EVALUATE CROSS-ENCODER PERFORMANCE
# =============================================================================

print("\nğŸ“Š Evaluating cross-encoder performance...")

# Load qrels
qrels_path = f"{PROJECT_DIR}/data/qrels.trec"
qrels_dict = {}
with open(qrels_path, 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 4:
            qid, _, docid, rel = parts
            if qid not in qrels_dict:
                qrels_dict[qid] = {}
            qrels_dict[qid][docid] = int(rel)

# Create run dict
run_dict = {}
for _, row in ce_runs_df.iterrows():
    qid = row['query_id']
    if qid not in run_dict:
        run_dict[qid] = {}
    run_dict[qid][row['doc_id']] = float(row['score'])

print(f"ğŸ“‹ Evaluation setup:")
print(f"   Qrels queries: {len(qrels_dict)}")
print(f"   Run queries: {len(run_dict)}")

# Calculate metrics
metrics = [nDCG@10, nDCG@20, MAP, MRR, P@10, Recall@100]
results = ir_measures.calc_aggregate(metrics, qrels_dict, run_dict)

print("\n" + "="*50)
print("ğŸ“ˆ CROSS-ENCODER RERANKING RESULTS")
print("="*50)

ce_metrics = {}
for metric in results:
    value = results[metric]
    ce_metrics[str(metric)] = value
    print(f"{str(metric)}: {value:.4f}")

# Compare with previous baselines
print(f"\nğŸ”„ Performance Comparison:")
bm25_ndcg10 = bm25_baseline['metrics'].get('nDCG@10', 0)
semantic_ndcg10 = semantic_baseline['metrics'].get('nDCG@10', 0)
ce_ndcg10 = ce_metrics.get('nDCG@10', 0)

print(f"   ğŸ“Š BM25 nDCG@10: {bm25_ndcg10:.4f}")
print(f"   ğŸ“Š Semantic nDCG@10: {semantic_ndcg10:.4f}")
print(f"   ğŸ“Š Cross-Encoder nDCG@10: {ce_ndcg10:.4f}")

# Calculate improvements
semantic_improvement = ((semantic_ndcg10 - bm25_ndcg10) / max(bm25_ndcg10, 0.0001)) * 100
ce_improvement = ((ce_ndcg10 - semantic_ndcg10) / max(semantic_ndcg10, 0.0001)) * 100
total_improvement = ((ce_ndcg10 - bm25_ndcg10) / max(bm25_ndcg10, 0.0001)) * 100

print(f"\nğŸ“ˆ Improvement Analysis:")
print(f"   BM25 â†’ Semantic: {semantic_improvement:+.1f}%")
print(f"   Semantic â†’ Cross-Encoder: {ce_improvement:+.1f}%")
print(f"   BM25 â†’ Cross-Encoder (Total): {total_improvement:+.1f}%")

if ce_ndcg10 > semantic_ndcg10:
    print(f"   âœ… Cross-encoder improves over semantic retrieval!")
else:
    print(f"   âš ï¸ Cross-encoder needs tuning")

# =============================================================================
# SECTION 10: TIMING AND EFFICIENCY ANALYSIS
# =============================================================================

print("\n" + "="*50)
print("â±ï¸ CROSS-ENCODER TIMING ANALYSIS")
print("="*50)

# Calculate per-query timing
avg_pairs_per_query = len(rerank_candidates) / len(queries_df)
avg_time_per_query = total_reranking_time / len(queries_df)
avg_time_per_pair = total_reranking_time / len(rerank_candidates)

print(f"Reranking performance:")
print(f"  Total reranking time: {total_reranking_time:.1f} seconds")
print(f"  Average pairs per query: {avg_pairs_per_query:.1f}")
print(f"  Average time per query: {avg_time_per_query:.3f} seconds")
print(f"  Average time per pair: {avg_time_per_pair:.4f} seconds")

# Throughput calculations
pairs_per_second = len(rerank_candidates) / total_reranking_time
queries_per_second = len(queries_df) / total_reranking_time

print(f"  Throughput: {pairs_per_second:.1f} pairs/second")
print(f"  Query throughput: {queries_per_second:.1f} queries/second")

print(f"\nğŸ“Š Pipeline Efficiency:")
print(f"  Candidates for reranking: {len(rerank_candidates):,}")
print(f"  Average reranking depth: {top_k_for_reranking}")
print(f"  Score correlation: {np.mean(reranking_stats):.3f}")

# =============================================================================
# SECTION 11: COMPREHENSIVE VISUALIZATIONS
# =============================================================================

print("\nğŸ“Š Creating comprehensive visualizations...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Performance comparison across all methods
methods = ['BM25', 'Semantic', 'Cross-Encoder']
ndcg10_scores = [bm25_ndcg10, semantic_ndcg10, ce_ndcg10]
colors = ['blue', 'green', 'red']

axes[0,0].bar(methods, ndcg10_scores, color=colors, alpha=0.7, edgecolor='black')
axes[0,0].set_title('nDCG@10 Performance Comparison')
axes[0,0].set_ylabel('nDCG@10 Score')
axes[0,0].set_ylim(0, max(ndcg10_scores) * 1.2)
for i, score in enumerate(ndcg10_scores):
    axes[0,0].text(i, score + max(ndcg10_scores) * 0.02, f'{score:.4f}', ha='center')

# Cross-encoder score distribution
axes[0,1].hist(ce_scores_array, bins=50, alpha=0.7, edgecolor='black', color='red')
axes[0,1].set_title('Cross-Encoder Score Distribution')
axes[0,1].set_xlabel('Cross-Encoder Score')
axes[0,1].set_ylabel('Frequency')

# Semantic vs Cross-encoder score scatter plot
semantic_scores_for_plot = [c['semantic_score'] for c in rerank_candidates]
ce_scores_for_plot = [c['ce_score'] for c in rerank_candidates]
axes[0,2].scatter(semantic_scores_for_plot, ce_scores_for_plot, alpha=0.5, s=1)
axes[0,2].set_title('Semantic vs Cross-Encoder Scores')
axes[0,2].set_xlabel('Semantic Score')
axes[0,2].set_ylabel('Cross-Encoder Score')

# Correlation distribution
axes[1,0].hist(reranking_stats, bins=30, alpha=0.7, edgecolor='black', color='purple')
axes[1,0].set_title('Semantic-CE Score Correlation Distribution')
axes[1,0].set_xlabel('Spearman Correlation')
axes[1,0].set_ylabel('Frequency')
axes[1,0].axvline(np.mean(reranking_stats), color='red', linestyle='--', label=f'Mean: {np.mean(reranking_stats):.3f}')
axes[1,0].legend()

# Performance improvement breakdown
improvement_stages = ['BM25â†’Semantic', 'Semanticâ†’CE', 'BM25â†’CE (Total)']
improvement_values = [semantic_improvement, ce_improvement, total_improvement]
colors_imp = ['green', 'red', 'purple']

axes[1,1].bar(improvement_stages, improvement_values, color=colors_imp, alpha=0.7, edgecolor='black')
axes[1,1].set_title('Performance Improvement Breakdown')
axes[1,1].set_ylabel('Improvement (%)')
axes[1,1].tick_params(axis='x', rotation=45)
for i, val in enumerate(improvement_values):
    axes[1,1].text(i, val + max(improvement_values) * 0.02, f'{val:+.1f}%', ha='center')

# Timing comparison (estimated)
timing_methods = ['BM25\n(~26ms)', 'Semantic\n(~7ms)', 'Cross-Encoder\n(~{}ms)'.format(int(avg_time_per_query * 1000))]
timing_values = [26, 7, avg_time_per_query * 1000]

axes[1,2].bar(timing_methods, timing_values, color=colors, alpha=0.7, edgecolor='black')
axes[1,2].set_title('Average Query Latency Comparison')
axes[1,2].set_ylabel('Latency (ms)')

plt.tight_layout()
plt.savefig(f"{PROJECT_DIR}/reports/cross_encoder_analysis.png", dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# SECTION 12: SAVE COMPREHENSIVE REPORT
# =============================================================================

print("\nğŸ’¾ Saving comprehensive cross-encoder report...")

cross_encoder_report = {
    'model': 'CrossEncoder_Reranking',
    'cross_encoder_model': model_name,
    'dataset': {
        'total_documents': len(corpus_df),
        'total_queries': len(queries_df),
        'reranking_candidates': len(rerank_candidates)
    },
    'performance': {
        'total_results': len(final_results),
        'queries_with_results': len(set([r['query_id'] for r in final_results])),
        'avg_candidates_per_query': avg_pairs_per_query,
        'reranking_depth': top_k_for_reranking
    },
    'metrics': ce_metrics,
    'timing': {
        'total_reranking_sec': total_reranking_time,
        'avg_time_per_query_sec': avg_time_per_query,
        'avg_time_per_pair_sec': avg_time_per_pair,
        'throughput_pairs_per_sec': pairs_per_second,
        'throughput_queries_per_sec': queries_per_second
    },
    'comparison': {
        'bm25_ndcg10': bm25_ndcg10,
        'semantic_ndcg10': semantic_ndcg10,
        'cross_encoder_ndcg10': ce_ndcg10,
        'semantic_improvement_percent': semantic_improvement,
        'ce_improvement_percent': ce_improvement,
        'total_improvement_percent': total_improvement
    },
    'reranking_analysis': {
        'avg_semantic_ce_correlation': np.mean(reranking_stats),
        'ce_score_statistics': {
            'min': float(np.min(ce_scores_array)),
            'max': float(np.max(ce_scores_array)),
            'mean': float(np.mean(ce_scores_array)),
            'std': float(np.std(ce_scores_array))
        }
    },
    'config': cross_encoder_config
}

with open(f"{PROJECT_DIR}/reports/cross_encoder_metrics.json", 'w') as f:
    json.dump(cross_encoder_report, f, indent=2)

print(f"âœ… Comprehensive report saved")

# =============================================================================
# SECTION 13: FINAL SUMMARY AND NEXT STEPS
# =============================================================================

# Memory cleanup
del rerank_candidates, all_ce_scores
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()

print("\n" + "="*60)
print("âœ… CROSS-ENCODER RERANKING COMPLETED SUCCESSFULLY!")
print("="*60)

print(f"ğŸ¯ Final Performance Summary:")
print(f"   ğŸ“Š nDCG@10: {ce_metrics.get('nDCG@10', 0):.4f}")
print(f"   ğŸ“Š nDCG@20: {ce_metrics.get('nDCG@20', 0):.4f}")
print(f"   ğŸ“Š MAP: {ce_metrics.get('MAP', 0):.4f}")
print(f"   ğŸ“Š MRR: {ce_metrics.get('MRR', 0):.4f}")
print(f"   ğŸ“Š P@10: {ce_metrics.get('P@10', 0):.4f}")

print(f"\nğŸ† Complete Pipeline Performance:")
print(f"   ğŸ¥‰ BM25 Baseline: {bm25_ndcg10:.4f}")
print(f"   ğŸ¥ˆ + Semantic Retrieval: {semantic_ndcg10:.4f} ({semantic_improvement:+.1f}%)")
print(f"   ğŸ¥‡ + Cross-Encoder Reranking: {ce_ndcg10:.4f} ({ce_improvement:+.1f}%)")
print(f"   ğŸš€ Total Improvement: {total_improvement:+.1f}%")

print(f"\nğŸ“Š System Performance:")
print(f"   ğŸ¯ Reranking Coverage: {len(set([r['query_id'] for r in final_results]))/len(queries_df)*100:.1f}%")
print(f"   â±ï¸ Reranking Latency: {avg_time_per_query*1000:.1f} ms/query")
print(f"   ğŸ”„ Score Correlation: {np.mean(reranking_stats):.3f}")
print(f"   ğŸ“¦ Reranking Depth: {top_k_for_reranking} candidates/query")

print(f"\nğŸ“ Final Files Generated:")
print(f"   ğŸ” TREC Run: runs/cross_encoder_rerank.trec")
print(f"   ğŸ“Š Metrics: reports/cross_encoder_metrics.json")
print(f"   ğŸ“ˆ Visualizations: reports/cross_encoder_analysis.png")
print(f"   ğŸ“‹ Results: runs/cross_encoder_results.csv")
print(f"   ğŸ¯ Candidates: runs/reranking_candidates.csv")

print(f"\nğŸ‰ Three-Stage Retrieval Pipeline Complete!")
print(f"   âœ… Stage 1: BM25 Baseline â†’ Lexical matching foundation")
print(f"   âœ… Stage 2: Semantic Retrieval â†’ Query intent understanding")
print(f"   âœ… Stage 3: Cross-Encoder Reranking â†’ Fine-grained relevance scoring")

print(f"\nğŸš€ Ready for Phase 5: Demo & Final Report!")
print(f"   ğŸ“± Interactive demo with all three stages")
print(f"   ğŸ“Š Comprehensive ablation analysis")
print(f"   ğŸ“ Technical documentation")

print("="*60)

