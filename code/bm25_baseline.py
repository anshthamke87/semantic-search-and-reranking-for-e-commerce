# -*- coding: utf-8 -*-
"""bm25_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15SEhu74ZFzh1GQthKobjDRJJBoo1GQET
"""

# =============================================================================
# NOTEBOOK 2: BM25 BASELINE IMPLEMENTATION
# Runtime: CPU (no GPU needed for this notebook)
# Estimated Time: 45-60 minutes
# =============================================================================

# =============================================================================
# SECTION 1: PACKAGE INSTALLATION AND SETUP
# =============================================================================

# Install required packages
!pip install rank-bm25 -q
!pip install ir_measures -q
!pip install pyyaml -q

print("âœ… Packages installed successfully!")

# =============================================================================
# SECTION 2: IMPORTS AND ENVIRONMENT SETUP
# =============================================================================

import pandas as pd
import numpy as np
import json
import os
import time
import gc
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import re
import yaml

# BM25 implementation
from rank_bm25 import BM25Okapi

# IR evaluation
import ir_measures
from ir_measures import *

# Basic utilities
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

PROJECT_DIR = '/content/drive/MyDrive/semantic-search'
print(f"âœ… Project directory: {PROJECT_DIR}")
print("âœ… All imports completed successfully!")

# =============================================================================
# SECTION 3: LOAD DATASET AND CONFIGURATION
# =============================================================================

print("ğŸ“‚ Loading dataset from Phase 1...")

queries_df = pd.read_csv(f"{PROJECT_DIR}/data/queries.csv")
corpus_df = pd.read_csv(f"{PROJECT_DIR}/data/corpus.csv")
qrels_df = pd.read_csv(f"{PROJECT_DIR}/data/qrels.csv")

print(f"âœ… Loaded:")
print(f"   ğŸ“‹ Queries: {len(queries_df)}")
print(f"   ğŸ“¦ Products: {len(corpus_df)}")
print(f"   ğŸ¯ Qrels: {len(qrels_df)}")

# Load configuration
with open(f"{PROJECT_DIR}/cfg/retrieval.yaml", 'r') as f:
    config = yaml.safe_load(f)

bm25_config = config['bm25']
print(f"ğŸ“‹ BM25 Config: top_k = {bm25_config['top_k']}")

# Show sample data
print(f"\nğŸ“ Sample queries:")
for i in range(3):
    print(f"   {queries_df.iloc[i]['query_id']}: {queries_df.iloc[i]['query']}")

print(f"\nğŸ“¦ Sample products:")
for i in range(3):
    product = corpus_df.iloc[i]
    print(f"   {product['product_id']}: {product['title']}")

# =============================================================================
# SECTION 4: TEXT PREPROCESSING AND CORPUS PREPARATION
# =============================================================================

print("ğŸ”§ Setting up text preprocessing...")

def preprocess_text(text):
    """Clean and tokenize text for BM25"""
    if pd.isna(text):
        return []

    # Convert to lowercase
    text = str(text).lower()

    # Remove punctuation, keep letters and numbers
    text = re.sub(r'[^\w\s]', ' ', text)

    # Split into tokens
    tokens = text.split()

    # Remove very short tokens (length 1)
    tokens = [token for token in tokens if len(token) > 1]

    return tokens

# Test preprocessing
sample_text = "Apple Wireless Black Headphones"
sample_tokens = preprocess_text(sample_text)
print(f"ğŸ“ Preprocessing test:")
print(f"   Input: '{sample_text}'")
print(f"   Output: {sample_tokens}")

print("\nğŸ“š Preparing corpus for BM25 indexing...")

# Create document collection
documents = []
doc_ids = []

for _, product in tqdm(corpus_df.iterrows(), total=len(corpus_df), desc="Processing products"):
    # Combine all text fields
    full_text = f"{product['title']} {product['description']} {product['brand']} {product['category']}"

    # Preprocess and tokenize
    tokens = preprocess_text(full_text)

    documents.append(tokens)
    doc_ids.append(product['product_id'])

print(f"âœ… Prepared {len(documents)} documents for indexing")
print(f"ğŸ“„ Sample document tokens: {documents[0][:10]}...")

# =============================================================================
# SECTION 5: BUILD BM25 INDEX
# =============================================================================

print("ğŸ”¨ Building BM25 index...")

start_time = time.time()

# Create BM25 index
bm25 = BM25Okapi(documents)

index_time = time.time() - start_time

print(f"âœ… BM25 index built successfully in {index_time:.2f} seconds")
print(f"ğŸ“Š Index statistics:")
print(f"   ğŸ“„ Documents: {len(documents)}")
print(f"   ğŸ“š Vocabulary size: {len(bm25.idf)} terms")

# Test with sample query
sample_query = queries_df.iloc[0]['query']
sample_query_tokens = preprocess_text(sample_query)
test_scores = bm25.get_scores(sample_query_tokens)
top_indices = np.argsort(test_scores)[::-1][:5]

print(f"\nğŸ§ª Test search for: '{sample_query}'")
print(f"   Query tokens: {sample_query_tokens}")
print(f"   Score statistics: max={np.max(test_scores):.4f}, mean={np.mean(test_scores):.4f}")
print(f"   Top 5 results:")
for i, idx in enumerate(top_indices):
    product = corpus_df.iloc[idx]
    print(f"     {i+1}. {doc_ids[idx]}: {product['title']} (score: {test_scores[idx]:.4f})")

# =============================================================================
# SECTION 6: RUN BM25 RETRIEVAL FOR ALL QUERIES
# =============================================================================

print("\nğŸ” Running BM25 retrieval for all queries...")

k = bm25_config['top_k']  # Retrieve top-500 as configured
all_results = []
retrieval_times = []

for _, query_row in tqdm(queries_df.iterrows(), total=len(queries_df), desc="BM25 Search"):
    query_id = query_row['query_id']
    query_text = query_row['query']

    # Preprocess query
    query_tokens = preprocess_text(query_text)

    # Skip if no tokens
    if len(query_tokens) == 0:
        continue

    # Measure query time
    start_time = time.time()

    # Get BM25 scores for all documents
    scores = bm25.get_scores(query_tokens)

    # Get top-k results
    top_indices = np.argsort(scores)[::-1][:k]

    query_time = time.time() - start_time
    retrieval_times.append(query_time * 1000)  # Convert to milliseconds

    # Store results with positive scores
    for rank, idx in enumerate(top_indices):
        if scores[idx] > 0:  # Only include documents with positive scores
            all_results.append({
                'query_id': query_id,
                'Q0': 'Q0',  # Standard TREC format
                'doc_id': doc_ids[idx],
                'rank': rank + 1,
                'score': scores[idx],
                'run_id': 'BM25'
            })

print(f"âœ… Completed BM25 retrieval for {len(queries_df)} queries")
print(f"ğŸ“Š Total results: {len(all_results)}")
print(f"ğŸ“Š Queries with results: {len(set([r['query_id'] for r in all_results]))}")
print(f"â±ï¸ Average query time: {np.mean(retrieval_times):.2f} ms")

# =============================================================================
# SECTION 7: SAVE BM25 RUN FILE
# =============================================================================

print("\nğŸ’¾ Saving BM25 run file...")

# Create run dataframe
bm25_runs_df = pd.DataFrame(all_results)

# Save in TREC format
trec_run_path = f"{PROJECT_DIR}/runs/bm25.trec"
with open(trec_run_path, 'w') as f:
    for _, row in bm25_runs_df.iterrows():
        f.write(f"{row['query_id']} {row['Q0']} {row['doc_id']} {row['rank']} {row['score']:.6f} {row['run_id']}\n")

print(f"âœ… BM25 run file saved: {trec_run_path}")

# Also save as CSV for analysis
bm25_runs_df.to_csv(f"{PROJECT_DIR}/runs/bm25_results.csv", index=False)

# Show sample results
if len(bm25_runs_df) > 0:
    sample_query_id = bm25_runs_df.iloc[0]['query_id']
    sample_results = bm25_runs_df[bm25_runs_df['query_id'] == sample_query_id].head(5)
    sample_query_text = queries_df[queries_df['query_id'] == sample_query_id].iloc[0]['query']

    print(f"\nğŸ“‹ Sample BM25 results for: '{sample_query_text}'")
    for _, row in sample_results.iterrows():
        product_info = corpus_df[corpus_df['product_id'] == row['doc_id']].iloc[0]
        print(f"   Rank {row['rank']}: {product_info['title']} (score: {row['score']:.3f})")

# =============================================================================
# SECTION 8: EVALUATE BM25 PERFORMANCE
# =============================================================================

print("\nğŸ“Š Evaluating BM25 performance...")

# Load qrels for evaluation
qrels_path = f"{PROJECT_DIR}/data/qrels.trec"
qrels_dict = {}
with open(qrels_path, 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 4:
            qid, _, docid, rel = parts
            if qid not in qrels_dict:
                qrels_dict[qid] = {}
            qrels_dict[qid][docid] = int(rel)

print(f"ğŸ“‹ Loaded qrels for {len(qrels_dict)} queries")

# Create run dict for ir_measures
run_dict = {}
for _, row in bm25_runs_df.iterrows():
    qid = row['query_id']
    if qid not in run_dict:
        run_dict[qid] = {}
    run_dict[qid][row['doc_id']] = float(row['score'])

print(f"ğŸ“‹ Loaded run results for {len(run_dict)} queries")

# Calculate metrics
print("ğŸ§® Computing evaluation metrics...")

# Primary metrics for information retrieval
metrics = [nDCG@10, nDCG@20, MAP, MRR, P@10, Recall@100]

# Evaluate
results = ir_measures.calc_aggregate(metrics, qrels_dict, run_dict)

print("\n" + "="*50)
print("ğŸ“ˆ BM25 BASELINE RESULTS")
print("="*50)

bm25_metrics = {}
for metric in results:
    value = results[metric]
    bm25_metrics[str(metric)] = value
    print(f"{str(metric)}: {value:.4f}")

# =============================================================================
# SECTION 9: TIMING ANALYSIS AND PERFORMANCE METRICS
# =============================================================================

print("\n" + "="*50)
print("â±ï¸ TIMING ANALYSIS")
print("="*50)

# Calculate timing statistics
retrieval_times_ms = np.array(retrieval_times)
timing_stats = {
    'mean_ms': np.mean(retrieval_times_ms),
    'median_ms': np.median(retrieval_times_ms),
    'p50_ms': np.percentile(retrieval_times_ms, 50),
    'p95_ms': np.percentile(retrieval_times_ms, 95),
    'p99_ms': np.percentile(retrieval_times_ms, 99),
    'min_ms': np.min(retrieval_times_ms),
    'max_ms': np.max(retrieval_times_ms),
    'std_ms': np.std(retrieval_times_ms)
}

print(f"Query latency statistics:")
print(f"  Mean: {timing_stats['mean_ms']:.2f} ms")
print(f"  Median (p50): {timing_stats['p50_ms']:.2f} ms")
print(f"  p95: {timing_stats['p95_ms']:.2f} ms")
print(f"  p99: {timing_stats['p99_ms']:.2f} ms")
print(f"  Min: {timing_stats['min_ms']:.2f} ms")
print(f"  Max: {timing_stats['max_ms']:.2f} ms")

# Estimate throughput
avg_latency_sec = timing_stats['mean_ms'] / 1000
estimated_qps = 1 / avg_latency_sec
print(f"\nğŸ“Š Estimated throughput: {estimated_qps:.1f} queries/second")

# =============================================================================
# SECTION 10: VISUALIZATIONS AND ANALYSIS
# =============================================================================

print("\nğŸ“Š Creating visualizations...")

# Create comprehensive analysis plots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Query latency distribution
axes[0,0].hist(retrieval_times_ms, bins=30, alpha=0.7, edgecolor='black')
axes[0,0].set_title('BM25 Query Latency Distribution')
axes[0,0].set_xlabel('Latency (ms)')
axes[0,0].set_ylabel('Frequency')
axes[0,0].axvline(timing_stats['p50_ms'], color='red', linestyle='--', label='p50')
axes[0,0].axvline(timing_stats['p95_ms'], color='orange', linestyle='--', label='p95')
axes[0,0].legend()

# Score distribution
all_scores = [float(score) for score in bm25_runs_df['score']]
axes[0,1].hist(all_scores, bins=50, alpha=0.7, edgecolor='black')
axes[0,1].set_title('BM25 Score Distribution')
axes[0,1].set_xlabel('BM25 Score')
axes[0,1].set_ylabel('Frequency')

# Results per query
results_per_query = bm25_runs_df.groupby('query_id').size()
axes[1,0].hist(results_per_query, bins=20, alpha=0.7, edgecolor='black')
axes[1,0].set_title('Results per Query Distribution')
axes[1,0].set_xlabel('Number of Results')
axes[1,0].set_ylabel('Frequency')

# Performance metrics bar chart
metrics_names = ['nDCG@10', 'nDCG@20', 'MAP', 'MRR', 'P@10']
metrics_values = [bm25_metrics.get(name, 0) for name in metrics_names]
axes[1,1].bar(metrics_names, metrics_values, alpha=0.7, edgecolor='black')
axes[1,1].set_title('BM25 Performance Metrics')
axes[1,1].set_ylabel('Score')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig(f"{PROJECT_DIR}/reports/bm25_baseline_analysis.png", dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# SECTION 11: SAVE COMPREHENSIVE REPORT
# =============================================================================

print("\nğŸ’¾ Saving comprehensive BM25 baseline report...")

# Calculate index size estimate
import sys
index_size_mb = sys.getsizeof(bm25) / 1024 / 1024

# Save detailed metrics report
bm25_report = {
    'model': 'BM25_Baseline',
    'dataset': {
        'total_documents': len(corpus_df),
        'total_queries': len(queries_df),
        'total_qrels': len(qrels_df)
    },
    'performance': {
        'total_results': len(all_results),
        'queries_with_results': len(set([r['query_id'] for r in all_results])),
        'success_rate': len(set([r['query_id'] for r in all_results])) / len(queries_df)
    },
    'metrics': bm25_metrics,
    'timing': timing_stats,
    'system': {
        'vocabulary_size': len(bm25.idf),
        'index_size_mb': index_size_mb,
        'index_build_time_sec': index_time
    },
    'config': bm25_config
}

with open(f"{PROJECT_DIR}/reports/bm25_baseline_metrics.json", 'w') as f:
    json.dump(bm25_report, f, indent=2)

print(f"âœ… Saved comprehensive report: reports/bm25_baseline_metrics.json")

# =============================================================================
# SECTION 12: SUMMARY AND NEXT STEPS (FIXED)
# =============================================================================

print("\n" + "="*60)
print("âœ… BM25 BASELINE COMPLETED SUCCESSFULLY!")
print("="*60)

print(f"ğŸ¯ BM25 Baseline Performance Summary:")
print(f"   ğŸ“Š nDCG@10: {bm25_metrics.get('nDCG@10', 0):.4f}")
print(f"   ğŸ“Š nDCG@20: {bm25_metrics.get('nDCG@20', 0):.4f}")
print(f"   ğŸ“Š MAP: {bm25_metrics.get('MAP', 0):.4f}")
print(f"   ğŸ“Š MRR: {bm25_metrics.get('MRR', 0):.4f}")
print(f"   ğŸ“Š P@10: {bm25_metrics.get('P@10', 0):.4f}")
print(f"   ğŸ“Š Recall@100: {bm25_metrics.get('Recall@100', 0):.4f}")

print(f"\nğŸ“Š System Performance:")
# Use bm25_report data instead of deleted variables
success_rate = bm25_report['performance']['success_rate'] * 100
queries_with_results = bm25_report['performance']['queries_with_results']
total_results = bm25_report['performance']['total_results']

print(f"   ğŸ” Query Success Rate: {success_rate:.1f}%")
print(f"   â±ï¸  p95 Latency: {timing_stats['p95_ms']:.2f} ms")
print(f"   ğŸ’¾ Index Size: {index_size_mb:.1f} MB")
print(f"   ğŸ“š Vocabulary: {len(bm25.idf)} terms")
print(f"   ğŸš€ Throughput: {estimated_qps:.1f} QPS")

print(f"\nğŸ“ Files Generated:")
print(f"   ğŸ” TREC Run File: runs/bm25.trec")
print(f"   ğŸ“Š Metrics Report: reports/bm25_baseline_metrics.json")
print(f"   ğŸ“ˆ Visualizations: reports/bm25_baseline_analysis.png")
print(f"   ğŸ“‹ Results CSV: runs/bm25_results.csv")

print(f"\nğŸ¯ Baseline Established:")
print(f"   âœ… Strong foundation for semantic retrieval comparison")
print(f"   âœ… {queries_with_results} queries returning results")
print(f"   âœ… {total_results} total candidate documents for reranking")

print(f"\nğŸš€ Ready for Phase 3: Semantic Retrieval (GPU runtime)")
print(f"   Expected improvements with bi-encoder + FAISS:")
print(f"   ğŸ“ˆ nDCG@10: {bm25_metrics.get('nDCG@10', 0):.4f} â†’ 0.05-0.15 (+200-500%)")
print(f"   ğŸ“ˆ Semantic understanding of query intent")
print(f"   ğŸ“ˆ Better handling of synonyms and related terms")

print("="*60)

# Memory cleanup (moved to end)
try:
    del documents
    print("âœ… Memory cleanup completed")
except:
    pass
gc.collect()

