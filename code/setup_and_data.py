# -*- coding: utf-8 -*-
"""setup_and_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1liBVfQTS_M7gkvYFrMaGSae4tq3nQ126
"""

# =============================================================================
# NOTEBOOK 1: SETUP & DATA PREPARATION
# Runtime: CPU (no GPU needed for this notebook)
# Estimated Time: 30-45 minutes
# =============================================================================

import pandas as pd
import numpy as np
import json
import os
import random
import gc
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print("‚úÖ Imports completed and random seeds set!")

# =============================================================================
# SECTION 3: GOOGLE DRIVE SETUP
# =============================================================================

from google.colab import drive
drive.mount('/content/drive')

# Create project directory structure
PROJECT_DIR = '/content/drive/MyDrive/semantic-search'
os.makedirs(PROJECT_DIR, exist_ok=True)

# Create subdirectories
subdirs = ['data', 'indices', 'runs', 'reports', 'cfg', 'notebooks', 'app']
for subdir in subdirs:
    os.makedirs(f"{PROJECT_DIR}/{subdir}", exist_ok=True)

print(f"‚úÖ Project structure created at: {PROJECT_DIR}")
print("üìÅ Directory structure:")
for subdir in subdirs:
    print(f"   ‚îú‚îÄ‚îÄ {subdir}/")

# =============================================================================
# SECTION 4: CONFIGURATION SETUP
# =============================================================================

# Dataset configuration
dataset_config = {
    'name': 'amazon_esci',
    'target_queries': 800,
    'target_products': 20000,
    'random_seed': RANDOM_SEED,
    'min_judgments_per_query': 5,
    'max_judgments_per_query': 100
}

# Save dataset config
with open(f"{PROJECT_DIR}/cfg/dataset.yaml", 'w') as f:
    import yaml
    yaml.dump(dataset_config, f)

# Retrieval configuration
retrieval_config = {
    'bm25': {
        'top_k': 500
    },
    'bi_encoder': {
        'model_name': 'all-MiniLM-L6-v2',
        'batch_size': 64,
        'top_k': 100
    },
    'faiss': {
        'index_type': 'HNSW',
        'M': 32,
        'efConstruction': 200,
        'efSearch': 64
    },
    'cross_encoder': {
        'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
        'batch_size': 32,
        'top_k': 50
    }
}

# Save retrieval config
with open(f"{PROJECT_DIR}/cfg/retrieval.yaml", 'w') as f:
    yaml.dump(retrieval_config, f)

print("‚úÖ Configuration files created!")

# =============================================================================
# SECTION 5: CREATE REALISTIC E-COMMERCE QUERIES (FIXED VERSION)
# =============================================================================

import urllib.request
import zipfile

print("üì• Creating realistic e-commerce dataset...")
print("üì¶ Generating queries that match our product catalog...")

# First, let's create our product catalog (we'll move this before query generation)
print("üè™ Creating product catalog first...")

product_categories = [
    "Electronics", "Computers", "Cell Phones", "Accessories",
    "Home & Kitchen", "Sports & Outdoors", "Health & Personal Care",
    "Automotive", "Tools & Home Improvement", "Office Products"
]

brands = [
    "Apple", "Samsung", "Sony", "HP", "Dell", "Logitech", "Anker",
    "AmazonBasics", "AUKEY", "RAVPower", "Jabra", "Bose", "JBL"
]

# Product types for realistic combinations
product_types = {
    "Electronics": ["headphones", "speakers", "charger", "cable", "adapter", "battery", "case"],
    "Computers": ["laptop", "mouse", "keyboard", "monitor", "webcam", "stand", "hub"],
    "Cell Phones": ["case", "screen protector", "charger", "mount", "holder", "cable"],
    "Accessories": ["cable", "adapter", "case", "holder", "mount", "organizer"],
    "Home & Kitchen": ["coffee maker", "water bottle", "organizer", "storage"],
    "Sports & Outdoors": ["fitness tracker", "water bottle", "equipment"],
    "Health & Personal Care": ["tracker", "monitor", "scale"],
    "Automotive": ["charger", "mount", "adapter", "cable"],
    "Tools & Home Improvement": ["organizer", "storage", "equipment"],
    "Office Products": ["organizer", "stand", "lamp", "chair", "desk"]
}

# Generate 20,000 products with realistic titles
products = []
for i in range(20000):
    category = random.choice(product_categories)
    brand = random.choice(brands)
    product_type = random.choice(product_types[category])

    # Create realistic product titles
    descriptors = ["wireless", "portable", "premium", "professional", "compact", "heavy duty", "universal"]
    colors = ["black", "white", "blue", "silver", "red"]

    # Sometimes add descriptors
    title_parts = [brand]
    if random.random() < 0.6:  # 60% chance to add descriptor
        title_parts.append(random.choice(descriptors))
    if random.random() < 0.4:  # 40% chance to add color
        title_parts.append(random.choice(colors))
    title_parts.append(product_type)

    product_title = " ".join(title_parts).title()

    product = {
        'product_id': f'B{i:07d}',
        'title': product_title,
        'description': f"High quality {product_type} from {brand}. Perfect for daily use with {category.lower()} devices.",
        'brand': brand,
        'category': category,
        'product_type': product_type,
        'price': round(random.uniform(10, 500), 2)
    }
    products.append(product)

print(f"‚úÖ Generated {len(products)} realistic products")

# Show sample products
print(f"\nüì¶ Sample realistic products:")
for i in range(5):
    p = products[i]
    print(f"   {p['product_id']}: {p['title']}")

# NOW create queries that will match these products
print(f"\nüìù Creating 800 queries that match our product catalog...")

enhanced_queries = []

# Strategy 1: Brand-based queries (simple brand names)
for brand in brands:
    enhanced_queries.append(brand.lower())
    enhanced_queries.append(f"{brand.lower()} products")

# Strategy 2: Category-based queries
for category in product_categories:
    enhanced_queries.append(category.lower().replace(' & ', ' ').replace('&', ''))
    enhanced_queries.append(f"{category.lower().split()[0]} products")

# Strategy 3: Product type queries
all_product_types = []
for types_list in product_types.values():
    all_product_types.extend(types_list)

for product_type in set(all_product_types):
    enhanced_queries.append(product_type)
    enhanced_queries.append(f"wireless {product_type}")
    enhanced_queries.append(f"portable {product_type}")
    enhanced_queries.append(f"best {product_type}")

# Strategy 4: Brand + Product type combinations
for brand in brands:
    for product_type in ["headphones", "mouse", "keyboard", "charger", "case", "cable"]:
        enhanced_queries.append(f"{brand.lower()} {product_type}")

# Strategy 5: Brand + Category combinations
for brand in brands[:8]:  # Use first 8 brands
    for category in ["electronics", "computers", "accessories"]:
        enhanced_queries.append(f"{brand.lower()} {category}")

# Strategy 6: Descriptive queries
descriptive_queries = [
    "wireless bluetooth headphones", "gaming mouse", "laptop charger", "phone case",
    "portable speaker", "computer monitor", "wireless keyboard", "usb cable",
    "bluetooth speaker", "laptop stand", "phone mount", "desk lamp",
    "office chair", "gaming headset", "portable charger", "wireless adapter",
    "computer speakers", "laptop bag", "phone charger", "desk organizer"
]

enhanced_queries.extend(descriptive_queries)

# Strategy 7: Add color + product combinations
colors = ["black", "white", "blue", "red", "silver"]
for color in colors:
    for product_type in ["headphones", "mouse", "keyboard", "case", "charger"]:
        enhanced_queries.append(f"{color} {product_type}")

# Strategy 8: Add descriptor + product combinations
descriptors = ["wireless", "portable", "premium", "professional", "compact"]
for descriptor in descriptors:
    for product_type in ["headphones", "mouse", "speaker", "charger", "adapter"]:
        enhanced_queries.append(f"{descriptor} {product_type}")

# Remove duplicates and ensure we have exactly 800
enhanced_queries = list(set(enhanced_queries))  # Remove duplicates
print(f"üìä Generated {len(enhanced_queries)} unique queries after deduplication")

# If we have more than 800, randomly sample 800
if len(enhanced_queries) > 800:
    enhanced_queries = random.sample(enhanced_queries, 800)
# If we have fewer than 800, pad with variations
elif len(enhanced_queries) < 800:
    while len(enhanced_queries) < 800:
        base_query = enhanced_queries[len(enhanced_queries) % len(enhanced_queries)]
        variations = [f"best {base_query}", f"cheap {base_query}", f"professional {base_query}"]
        for variation in variations:
            if len(enhanced_queries) < 800:
                enhanced_queries.append(variation)

# Final trim to exactly 800
enhanced_queries = enhanced_queries[:800]

print(f"‚úÖ Final query set: {len(enhanced_queries)} queries")

# Show sample queries
print(f"\nüìù Sample enhanced queries:")
for i in range(10):
    print(f"   {i+1}: {enhanced_queries[i]}")

# Verify query-product matching potential
print(f"\nüîç Testing query-product matching:")
sample_product = products[0]
sample_query = enhanced_queries[0]
print(f"   Sample product: {sample_product['title']}")
print(f"   Sample query: {sample_query}")

# Check for word overlap
product_words = set(sample_product['title'].lower().split())
query_words = set(sample_query.lower().split())
overlap = product_words.intersection(query_words)
print(f"   Word overlap: {overlap}")

if len(overlap) > 0:
    print(f"   ‚úÖ Good! Queries should match products well")
else:
    print(f"   ‚ö†Ô∏è Testing another pair...")
    # Test with a brand-based query
    brand_query = brands[0].lower()
    brand_products = [p for p in products if p['brand'].lower() == brand_query]
    print(f"   Brand query '{brand_query}' matches {len(brand_products)} products")

print(f"\nüéØ Query generation strategy summary:")
print(f"   üìä Brand queries: ~{len(brands) * 2}")
print(f"   üìä Category queries: ~{len(product_categories) * 2}")
print(f"   üìä Product type queries: ~{len(set(all_product_types)) * 4}")
print(f"   üìä Brand+Product combinations: ~{len(brands) * 6}")
print(f"   üìä Descriptive queries: ~{len(descriptive_queries)}")
print(f"   üìä Color+Product combinations: ~{len(colors) * 5}")
print(f"   üìä Descriptor+Product combinations: ~{len(descriptors) * 5}")
print(f"   üìä Total unique queries: {len(enhanced_queries)}")

# Store both products and queries for next sections
print(f"‚úÖ Ready for Section 6: Creating relevance judgments with better matching!")

# =============================================================================
# SECTION 6: PRODUCT CATALOG SUMMARY (ALREADY CREATED IN SECTION 5)
# =============================================================================

print("üì¶ Product catalog already created in Section 5!")
print(f"‚úÖ Generated {len(products)} products")

# Show detailed product statistics
brands_count = {}
categories_count = {}
types_count = {}

for product in products:
    brands_count[product['brand']] = brands_count.get(product['brand'], 0) + 1
    categories_count[product['category']] = categories_count.get(product['category'], 0) + 1
    types_count[product['product_type']] = types_count.get(product['product_type'], 0) + 1

print(f"\nüìä Product distribution:")
print(f"   üè∑Ô∏è Brands: {len(brands_count)} unique brands")
print(f"   üìÇ Categories: {len(categories_count)} unique categories")
print(f"   üîß Product types: {len(types_count)} unique types")

print(f"\nüèÜ Top 5 brands by product count:")
top_brands = sorted(brands_count.items(), key=lambda x: x[1], reverse=True)[:5]
for brand, count in top_brands:
    print(f"   {brand}: {count} products")

print(f"\nüìÇ Category distribution:")
for category, count in sorted(categories_count.items()):
    print(f"   {category}: {count} products")

print(f"\n‚úÖ Product catalog ready for relevance judgment creation!")

# =============================================================================
# SECTION 7: CREATE QUERY-PRODUCT RELEVANCE JUDGMENTS (FIXED)
# =============================================================================

print("üéØ Creating relevance judgments...")

# ESCI labels: 0=Irrelevant, 1=Complement, 2=Substitute, 3=Exact
relevance_judgments = []

for query_idx, query in enumerate(tqdm(enhanced_queries, desc="Creating judgments")):  # FIXED: enhanced_queries
    query_id = f"Q{query_idx:06d}"

    # Sample 30-50 products per query
    num_judgments = random.randint(30, 50)
    sampled_products = random.sample(products, num_judgments)

    for product in sampled_products:
        # IMPROVED relevance simulation based on realistic matching
        query_words = set(query.lower().split())
        product_words = set(product['title'].lower().split())
        brand_words = set(product['brand'].lower().split())
        category_words = set(product['category'].lower().split())
        type_words = set(product['product_type'].lower().split())

        # Calculate different types of overlap
        title_overlap = len(query_words.intersection(product_words))
        brand_overlap = len(query_words.intersection(brand_words))
        category_overlap = len(query_words.intersection(category_words))
        type_overlap = len(query_words.intersection(type_words))

        # Better relevance logic
        if brand_overlap > 0 and (type_overlap > 0 or title_overlap >= 2):
            label = 3  # Exact: brand match + product type match
        elif brand_overlap > 0:
            label = 2  # Substitute: same brand, different product
        elif type_overlap > 0 or title_overlap >= 2:
            label = 2  # Substitute: same product type, different brand
        elif category_overlap > 0 or title_overlap == 1:
            label = 1  # Complement: related category or some overlap
        else:
            label = 0  # Irrelevant: no meaningful overlap

        relevance_judgments.append({
            'query_id': query_id,
            'query': query,
            'product_id': product['product_id'],
            'esci_label': label,
            'relevance': 1 if label > 0 else 0  # Binary relevance for IR metrics
        })

print(f"‚úÖ Created {len(relevance_judgments)} relevance judgments with improved matching logic")

# Show sample judgments
print(f"\nüìä Sample relevance judgments:")
for i in range(5):
    judgment = relevance_judgments[i]
    product = next(p for p in products if p['product_id'] == judgment['product_id'])
    print(f"   Query: '{judgment['query']}'")
    print(f"   Product: '{product['title']}'")
    print(f"   Label: {judgment['esci_label']} ({'Exact' if judgment['esci_label']==3 else 'Substitute' if judgment['esci_label']==2 else 'Complement' if judgment['esci_label']==1 else 'Irrelevant'})")
    print(f"   ---")

# =============================================================================
# SECTION 8: SAVE DATASET FILES (FIXED)
# =============================================================================

print("üíæ Saving dataset files...")

# Save queries (using enhanced_queries instead of extended_queries)
queries_df = pd.DataFrame([
    {'query_id': f"Q{i:06d}", 'query': query}
    for i, query in enumerate(enhanced_queries)  # FIXED: enhanced_queries
])
queries_df.to_csv(f"{PROJECT_DIR}/data/queries.csv", index=False)

# Save products (corpus) - using our new products variable
products_df = pd.DataFrame(products)  # FIXED: using products from Section 5
products_df.to_csv(f"{PROJECT_DIR}/data/corpus.csv", index=False)

# Save relevance judgments (qrels)
qrels_df = pd.DataFrame(relevance_judgments)
qrels_df.to_csv(f"{PROJECT_DIR}/data/qrels.csv", index=False)

# Save in TREC qrels format for ir_measures
trec_qrels = []
for _, row in qrels_df.iterrows():
    if row['relevance'] > 0:  # Only include relevant docs
        trec_qrels.append(f"{row['query_id']} 0 {row['product_id']} {row['esci_label']}")

with open(f"{PROJECT_DIR}/data/qrels.trec", 'w') as f:
    f.write('\n'.join(trec_qrels))

print("‚úÖ All dataset files saved!")

# Verify file contents
print(f"\nüìÅ Files created:")
print(f"   üìã queries.csv: {len(queries_df)} queries")
print(f"   üì¶ corpus.csv: {len(products_df)} products")
print(f"   üéØ qrels.csv: {len(qrels_df)} judgments")
print(f"   üéØ qrels.trec: {len(trec_qrels)} relevant judgments")

# =============================================================================
# SECTION 9: DATASET STATISTICS AND EXPLORATION (FIXED)
# =============================================================================

print("\n" + "="*50)
print("üìä DATASET STATISTICS")
print("="*50)

print(f"üìã Queries: {len(queries_df)}")
print(f"üì¶ Products: {len(products_df)}")
print(f"üéØ Relevance Judgments: {len(qrels_df)}")
print(f"üìà Avg Judgments per Query: {len(qrels_df) / len(queries_df):.1f}")

# Relevance distribution
relevance_dist = qrels_df['esci_label'].value_counts().sort_index()
print(f"\nüè∑Ô∏è ESCI Label Distribution:")
labels = {0: 'Irrelevant', 1: 'Complement', 2: 'Substitute', 3: 'Exact'}
for label, count in relevance_dist.items():
    print(f"   {label} ({labels[label]}): {count} ({count/len(qrels_df)*100:.1f}%)")

# Sample queries and products
print(f"\nüìù Sample Queries:")
for i in range(5):
    print(f"   {queries_df.iloc[i]['query_id']}: {queries_df.iloc[i]['query']}")

print(f"\nüì¶ Sample Products:")
for i in range(3):
    product = products_df.iloc[i]
    print(f"   {product['product_id']}: {product['title']}")

# Show some example query-product matches
print(f"\nüéØ Sample Query-Product Relevance Examples:")
sample_relevant = qrels_df[qrels_df['esci_label'] == 3].head(3)  # Show Exact matches
for _, rel in sample_relevant.iterrows():
    query_text = queries_df[queries_df['query_id'] == rel['query_id']].iloc[0]['query']
    product_info = products_df[products_df['product_id'] == rel['product_id']].iloc[0]
    print(f"   Query: '{query_text}' ‚Üí Product: '{product_info['title']}' (Exact match)")

# =============================================================================
# SECTION 10: VISUALIZATION
# =============================================================================

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# ESCI label distribution
axes[0,0].bar(relevance_dist.index, relevance_dist.values)
axes[0,0].set_title('ESCI Label Distribution')
axes[0,0].set_xlabel('ESCI Label')
axes[0,0].set_ylabel('Count')
axes[0,0].set_xticks(range(4))
axes[0,0].set_xticklabels(['Irrelevant', 'Complement', 'Substitute', 'Exact'])

# Judgments per query distribution
judgments_per_query = qrels_df.groupby('query_id').size()
axes[0,1].hist(judgments_per_query, bins=20, alpha=0.7)
axes[0,1].set_title('Judgments per Query Distribution')
axes[0,1].set_xlabel('Number of Judgments')
axes[0,1].set_ylabel('Frequency')

# Category distribution
category_dist = products_df['category'].value_counts()
axes[1,0].bar(range(len(category_dist)), category_dist.values)
axes[1,0].set_title('Product Category Distribution')
axes[1,0].set_xlabel('Category')
axes[1,0].set_ylabel('Count')
axes[1,0].set_xticks(range(len(category_dist)))
axes[1,0].set_xticklabels(category_dist.index, rotation=45, ha='right')

# Price distribution
axes[1,1].hist(products_df['price'], bins=30, alpha=0.7)
axes[1,1].set_title('Product Price Distribution')
axes[1,1].set_xlabel('Price ($)')
axes[1,1].set_ylabel('Frequency')

plt.tight_layout()
plt.savefig(f"{PROJECT_DIR}/reports/dataset_overview.png", dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# SECTION 11: MEMORY CLEANUP AND SUMMARY (FIXED)
# =============================================================================

# Clean up large variables
del relevance_judgments, products, enhanced_queries  # FIXED: enhanced_queries
gc.collect()

print("\n" + "="*50)
print("‚úÖ PHASE 1 COMPLETED SUCCESSFULLY!")
print("="*50)
print(f"üìÅ Project directory: {PROJECT_DIR}")
print(f"üìä Dataset ready: 800 queries, 20K products, ~{len(qrels_df)} judgments")
print(f"üíæ Files saved to data/ directory")
print(f"üìà Visualizations saved to reports/")
print(f"\nüéØ Dataset Quality Improvements:")
print(f"   ‚úÖ Realistic e-commerce queries")
print(f"   ‚úÖ Product-query matching guaranteed")
print(f"   ‚úÖ 8 strategic query types")
print(f"   ‚úÖ Improved relevance logic")
print("\nüöÄ Ready for Phase 2: BM25 Baseline (CPU runtime)")
print("="*50)

