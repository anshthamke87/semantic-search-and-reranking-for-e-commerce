# -*- coding: utf-8 -*-
"""semantic_retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fcX5MDq3099kY9drO9p9sR_FiDPfkjNy
"""

# =============================================================================
# NOTEBOOK 3: SEMANTIC RETRIEVAL WITH BI-ENCODER + FAISS
# Runtime: GPU (T4) - IMPORTANT: Switch to GPU runtime!
# Estimated Time: 60-90 minutes
# =============================================================================

# =============================================================================
# SECTION 1: PACKAGE INSTALLATION
# =============================================================================

# Install required packages for semantic retrieval
!pip install sentence-transformers -q
!pip install faiss-cpu -q  # GPU version for faster indexing
!pip install ir_measures -q
!pip install pyyaml -q

print("✅ Packages installed successfully!")

# =============================================================================
# SECTION 2: IMPORTS AND SETUP
# =============================================================================

import pandas as pd
import numpy as np
import json
import os
import time
import gc
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import yaml

# Sentence Transformers for bi-encoder
from sentence_transformers import SentenceTransformer

# FAISS for approximate nearest neighbor search
import faiss

# IR evaluation
import ir_measures
from ir_measures import *

# PyTorch utilities
import torch
print(f"🔥 GPU Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"🎯 GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

PROJECT_DIR = '/content/drive/MyDrive/semantic-search'
print(f"✅ Project directory: {PROJECT_DIR}")

# =============================================================================
# SECTION 3: LOAD DATASET AND BM25 BASELINE
# =============================================================================

print("📂 Loading dataset and BM25 baseline...")

# Load dataset
queries_df = pd.read_csv(f"{PROJECT_DIR}/data/queries.csv")
corpus_df = pd.read_csv(f"{PROJECT_DIR}/data/corpus.csv")
qrels_df = pd.read_csv(f"{PROJECT_DIR}/data/qrels.csv")

print(f"✅ Dataset loaded:")
print(f"   📋 Queries: {len(queries_df)}")
print(f"   📦 Products: {len(corpus_df)}")
print(f"   🎯 Qrels: {len(qrels_df)}")

# Load BM25 baseline results for comparison
try:
    with open(f"{PROJECT_DIR}/reports/bm25_baseline_metrics.json", 'r') as f:
        bm25_baseline = json.load(f)
    print(f"📊 BM25 Baseline nDCG@10: {bm25_baseline['metrics'].get('nDCG@10', 0):.4f}")
except:
    print("⚠️ BM25 baseline not found - will establish semantic baseline")
    bm25_baseline = {'metrics': {'nDCG@10': 0.0025}}  # Use your known result

# Load configuration
with open(f"{PROJECT_DIR}/cfg/retrieval.yaml", 'r') as f:
    config = yaml.safe_load(f)

bi_encoder_config = config['bi_encoder']
faiss_config = config['faiss']

print(f"🧠 Bi-encoder config: {bi_encoder_config}")
print(f"🔍 FAISS config: {faiss_config}")

# =============================================================================
# SECTION 4: LOAD BI-ENCODER MODEL
# =============================================================================

print("🧠 Loading bi-encoder model...")

model_name = bi_encoder_config['model_name']
print(f"📥 Loading model: {model_name}")

# Load sentence transformer model
bi_encoder = SentenceTransformer(model_name)

# Move to GPU if available
if torch.cuda.is_available():
    bi_encoder = bi_encoder.to('cuda')
    print("🔥 Model moved to GPU")

print(f"✅ Bi-encoder loaded successfully")
print(f"📏 Embedding dimension: {bi_encoder.get_sentence_embedding_dimension()}")

# Test encoding with sample text
sample_text = "wireless bluetooth headphones"
sample_embedding = bi_encoder.encode([sample_text])
print(f"🧪 Sample encoding test:")
print(f"   Input: '{sample_text}'")
print(f"   Output shape: {sample_embedding.shape}")
print(f"   Sample values: {sample_embedding[0][:5]}...")

# =============================================================================
# SECTION 5: PREPARE CORPUS TEXTS FOR ENCODING
# =============================================================================

print("📚 Preparing corpus texts for encoding...")

# Create document texts (similar to BM25 but keep original formatting)
corpus_texts = []
corpus_ids = []

for _, product in tqdm(corpus_df.iterrows(), total=len(corpus_df), desc="Preparing texts"):
    # Combine fields with better formatting for semantic understanding
    text = f"{product['title']}. {product['description']} Brand: {product['brand']}. Category: {product['category']}."

    corpus_texts.append(text)
    corpus_ids.append(product['product_id'])

print(f"✅ Prepared {len(corpus_texts)} corpus texts")

# Show sample texts
print(f"\n📄 Sample corpus texts:")
for i in range(3):
    print(f"   {i+1}. {corpus_texts[i][:100]}...")

# =============================================================================
# SECTION 6: ENCODE CORPUS WITH BI-ENCODER
# =============================================================================

print("🔮 Encoding corpus with bi-encoder...")

batch_size = bi_encoder_config['batch_size']
print(f"📦 Batch size: {batch_size}")

start_time = time.time()

# Encode corpus in batches to manage memory
corpus_embeddings = []
num_batches = (len(corpus_texts) + batch_size - 1) // batch_size

for i in tqdm(range(0, len(corpus_texts), batch_size), desc="Encoding corpus"):
    batch_texts = corpus_texts[i:i + batch_size]

    # Encode batch
    batch_embeddings = bi_encoder.encode(
        batch_texts,
        convert_to_tensor=False,
        show_progress_bar=False,
        batch_size=batch_size
    )

    corpus_embeddings.append(batch_embeddings)

    # Clear GPU cache periodically
    if torch.cuda.is_available() and i % (batch_size * 10) == 0:
        torch.cuda.empty_cache()

# Combine all embeddings
corpus_embeddings = np.vstack(corpus_embeddings)
encoding_time = time.time() - start_time

print(f"✅ Corpus encoding completed in {encoding_time:.1f} seconds")
print(f"📊 Corpus embeddings shape: {corpus_embeddings.shape}")
print(f"💾 Embeddings size: {corpus_embeddings.nbytes / 1024 / 1024:.1f} MB")

# Save embeddings to avoid re-computation
embeddings_path = f"{PROJECT_DIR}/indices/corpus_embeddings.npy"
os.makedirs(os.path.dirname(embeddings_path), exist_ok=True)
np.save(embeddings_path, corpus_embeddings)
print(f"💾 Corpus embeddings saved to: {embeddings_path}")

# =============================================================================
# SECTION 7: BUILD FAISS INDEX
# =============================================================================

print("🔍 Building FAISS index...")

# Get embedding dimension
embedding_dim = corpus_embeddings.shape[1]
print(f"📏 Embedding dimension: {embedding_dim}")

# Build HNSW index for fast approximate search
M = faiss_config['M']  # Number of connections per node
efConstruction = faiss_config['efConstruction']  # Search effort during construction
efSearch = faiss_config['efSearch']  # Search effort during query

print(f"🏗️ HNSW parameters: M={M}, efConstruction={efConstruction}, efSearch={efSearch}")

start_time = time.time()

# Create HNSW index
faiss_index = faiss.IndexHNSWFlat(embedding_dim, M)
faiss_index.hnsw.efConstruction = efConstruction
faiss_index.hnsw.efSearch = efSearch

# Add embeddings to index
print("📊 Adding embeddings to FAISS index...")
faiss_index.add(corpus_embeddings.astype('float32'))

index_build_time = time.time() - start_time

print(f"✅ FAISS index built in {index_build_time:.1f} seconds")
print(f"📊 Index statistics:")
print(f"   📄 Total vectors: {faiss_index.ntotal}")
print(f"   💾 Index size: ~{faiss_index.ntotal * embedding_dim * 4 / 1024 / 1024:.1f} MB")

# Save FAISS index
index_path = f"{PROJECT_DIR}/indices/faiss_hnsw"
os.makedirs(index_path, exist_ok=True)
faiss.write_index(faiss_index, f"{index_path}/index.faiss")

# Save mapping from FAISS indices to document IDs
with open(f"{index_path}/doc_ids.json", 'w') as f:
    json.dump(corpus_ids, f)

print(f"💾 FAISS index saved to: {index_path}")

# Test search
sample_query = "apple wireless headphones"
sample_query_embedding = bi_encoder.encode([sample_query])
scores, indices = faiss_index.search(sample_query_embedding.astype('float32'), 5)

print(f"\n🧪 Test search for: '{sample_query}'")
for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
    product_id = corpus_ids[idx]
    product = corpus_df[corpus_df['product_id'] == product_id].iloc[0]
    print(f"   {i+1}. {product['title']} (similarity: {score:.4f})")

# =============================================================================
# SECTION 8: RUN SEMANTIC RETRIEVAL FOR ALL QUERIES
# =============================================================================

print("🔍 Running semantic retrieval for all queries...")

top_k = bi_encoder_config['top_k']  # Retrieve top-100 for semantic
print(f"📊 Retrieving top-{top_k} results per query")

all_results = []
retrieval_times = []

for _, query_row in tqdm(queries_df.iterrows(), total=len(queries_df), desc="Semantic Search"):
    query_id = query_row['query_id']
    query_text = query_row['query']

    # Encode query
    start_time = time.time()
    query_embedding = bi_encoder.encode([query_text])

    # Search FAISS index
    similarities, indices = faiss_index.search(query_embedding.astype('float32'), top_k)

    query_time = time.time() - start_time
    retrieval_times.append(query_time * 1000)  # Convert to milliseconds

    # Store results
    for rank, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
        if idx != -1:  # Valid result
            all_results.append({
                'query_id': query_id,
                'Q0': 'Q0',
                'doc_id': corpus_ids[idx],
                'rank': rank + 1,
                'score': float(similarity),
                'run_id': 'BiEncoder'
            })

print(f"✅ Completed semantic retrieval for {len(queries_df)} queries")
print(f"📊 Total results: {len(all_results)}")
print(f"📊 Queries with results: {len(set([r['query_id'] for r in all_results]))}")
print(f"⏱️ Average query time: {np.mean(retrieval_times):.2f} ms")

# =============================================================================
# SECTION 9: SAVE SEMANTIC RETRIEVAL RESULTS
# =============================================================================

print("💾 Saving semantic retrieval results...")

# Create results dataframe
semantic_runs_df = pd.DataFrame(all_results)

# Save in TREC format
trec_run_path = f"{PROJECT_DIR}/runs/bi_encoder_ann.trec"
with open(trec_run_path, 'w') as f:
    for _, row in semantic_runs_df.iterrows():
        f.write(f"{row['query_id']} {row['Q0']} {row['doc_id']} {row['rank']} {row['score']:.6f} {row['run_id']}\n")

print(f"✅ Semantic run file saved: {trec_run_path}")

# Save as CSV for analysis
semantic_runs_df.to_csv(f"{PROJECT_DIR}/runs/bi_encoder_results.csv", index=False)

# Show sample results
sample_query_id = semantic_runs_df.iloc[0]['query_id']
sample_results = semantic_runs_df[semantic_runs_df['query_id'] == sample_query_id].head(10)
sample_query_text = queries_df[queries_df['query_id'] == sample_query_id].iloc[0]['query']

print(f"\n📋 Sample semantic results for: '{sample_query_text}'")
for _, row in sample_results.iterrows():
    product = corpus_df[corpus_df['product_id'] == row['doc_id']].iloc[0]
    print(f"   Rank {row['rank']}: {product['title']} (similarity: {row['score']:.4f})")

# =============================================================================
# SECTION 10: EVALUATE SEMANTIC RETRIEVAL
# =============================================================================

print("\n📊 Evaluating semantic retrieval performance...")

# Load qrels
qrels_path = f"{PROJECT_DIR}/data/qrels.trec"
qrels_dict = {}
with open(qrels_path, 'r') as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) == 4:
            qid, _, docid, rel = parts
            if qid not in qrels_dict:
                qrels_dict[qid] = {}
            qrels_dict[qid][docid] = int(rel)

# Create run dict
run_dict = {}
for _, row in semantic_runs_df.iterrows():
    qid = row['query_id']
    if qid not in run_dict:
        run_dict[qid] = {}
    run_dict[qid][row['doc_id']] = float(row['score'])

print(f"📋 Evaluation setup:")
print(f"   Qrels queries: {len(qrels_dict)}")
print(f"   Run queries: {len(run_dict)}")

# Calculate metrics
metrics = [nDCG@10, nDCG@20, MAP, MRR, P@10, Recall@100]
results = ir_measures.calc_aggregate(metrics, qrels_dict, run_dict)

print("\n" + "="*50)
print("📈 SEMANTIC RETRIEVAL RESULTS")
print("="*50)

semantic_metrics = {}
for metric in results:
    value = results[metric]
    semantic_metrics[str(metric)] = value
    print(f"{str(metric)}: {value:.4f}")

# Compare with BM25 baseline
print(f"\n🔄 Comparison with BM25 Baseline:")
bm25_ndcg10 = bm25_baseline['metrics'].get('nDCG@10', 0)
semantic_ndcg10 = semantic_metrics.get('nDCG@10', 0)
improvement = ((semantic_ndcg10 - bm25_ndcg10) / max(bm25_ndcg10, 0.0001)) * 100

print(f"   📊 BM25 nDCG@10: {bm25_ndcg10:.4f}")
print(f"   📊 Semantic nDCG@10: {semantic_ndcg10:.4f}")
print(f"   📈 Improvement: {improvement:+.1f}%")

if semantic_ndcg10 > bm25_ndcg10:
    print(f"   ✅ Semantic retrieval outperforms BM25!")
else:
    print(f"   ⚠️ Semantic retrieval needs tuning")

# =============================================================================
# SECTION 11: TIMING AND PERFORMANCE ANALYSIS
# =============================================================================

print("\n" + "="*50)
print("⏱️ SEMANTIC RETRIEVAL TIMING ANALYSIS")
print("="*50)

# Calculate timing statistics
retrieval_times_ms = np.array(retrieval_times)
timing_stats = {
    'mean_ms': np.mean(retrieval_times_ms),
    'p50_ms': np.percentile(retrieval_times_ms, 50),
    'p95_ms': np.percentile(retrieval_times_ms, 95),
    'p99_ms': np.percentile(retrieval_times_ms, 99),
    'min_ms': np.min(retrieval_times_ms),
    'max_ms': np.max(retrieval_times_ms)
}

print(f"Query latency statistics:")
print(f"  Mean: {timing_stats['mean_ms']:.2f} ms")
print(f"  p50: {timing_stats['p50_ms']:.2f} ms")
print(f"  p95: {timing_stats['p95_ms']:.2f} ms")
print(f"  p99: {timing_stats['p99_ms']:.2f} ms")

# Performance breakdown
print(f"\n📊 Performance Breakdown:")
print(f"   🔮 Corpus encoding: {encoding_time:.1f} seconds")
print(f"   🏗️ Index building: {index_build_time:.1f} seconds")
print(f"   🔍 Average query: {timing_stats['mean_ms']:.2f} ms")

# Throughput estimation
avg_latency_sec = timing_stats['mean_ms'] / 1000
estimated_qps = 1 / avg_latency_sec
print(f"   🚀 Estimated throughput: {estimated_qps:.1f} QPS")

# =============================================================================
# SECTION 12: VISUALIZATIONS
# =============================================================================

print("\n📊 Creating semantic retrieval visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Query latency distribution
axes[0,0].hist(retrieval_times_ms, bins=30, alpha=0.7, edgecolor='black', color='green')
axes[0,0].set_title('Semantic Retrieval Latency Distribution')
axes[0,0].set_xlabel('Latency (ms)')
axes[0,0].set_ylabel('Frequency')
axes[0,0].axvline(timing_stats['p50_ms'], color='red', linestyle='--', label='p50')
axes[0,0].axvline(timing_stats['p95_ms'], color='orange', linestyle='--', label='p95')
axes[0,0].legend()

# Similarity score distribution
all_scores = [float(score) for score in semantic_runs_df['score']]
axes[0,1].hist(all_scores, bins=50, alpha=0.7, edgecolor='black', color='green')
axes[0,1].set_title('Semantic Similarity Score Distribution')
axes[0,1].set_xlabel('Cosine Similarity')
axes[0,1].set_ylabel('Frequency')

# Results per query
results_per_query = semantic_runs_df.groupby('query_id').size()
axes[1,0].hist(results_per_query, bins=20, alpha=0.7, edgecolor='black', color='green')
axes[1,0].set_title('Results per Query Distribution')
axes[1,0].set_xlabel('Number of Results')
axes[1,0].set_ylabel('Frequency')

# Performance comparison: BM25 vs Semantic
metrics_names = ['nDCG@10', 'nDCG@20', 'MAP', 'MRR', 'P@10']
bm25_values = [bm25_baseline['metrics'].get(name, 0) for name in metrics_names]
semantic_values = [semantic_metrics.get(name, 0) for name in metrics_names]

x = np.arange(len(metrics_names))
width = 0.35

axes[1,1].bar(x - width/2, bm25_values, width, label='BM25', alpha=0.7, color='blue')
axes[1,1].bar(x + width/2, semantic_values, width, label='Semantic', alpha=0.7, color='green')
axes[1,1].set_title('BM25 vs Semantic Retrieval Comparison')
axes[1,1].set_ylabel('Score')
axes[1,1].set_xticks(x)
axes[1,1].set_xticklabels(metrics_names, rotation=45)
axes[1,1].legend()

plt.tight_layout()
plt.savefig(f"{PROJECT_DIR}/reports/semantic_retrieval_analysis.png", dpi=150, bbox_inches='tight')
plt.show()

# =============================================================================
# SECTION 13: SAVE COMPREHENSIVE REPORT
# =============================================================================

print("\n💾 Saving semantic retrieval report...")

# Calculate memory usage
corpus_embeddings_size_mb = corpus_embeddings.nbytes / 1024 / 1024
faiss_index_size_mb = faiss_index.ntotal * embedding_dim * 4 / 1024 / 1024

semantic_report = {
    'model': 'BiEncoder_Semantic',
    'bi_encoder_model': model_name,
    'embedding_dimension': embedding_dim,
    'dataset': {
        'total_documents': len(corpus_df),
        'total_queries': len(queries_df)
    },
    'performance': {
        'total_results': len(all_results),
        'queries_with_results': len(set([r['query_id'] for r in all_results])),
        'success_rate': len(set([r['query_id'] for r in all_results])) / len(queries_df)
    },
    'metrics': semantic_metrics,
    'timing': {
        'corpus_encoding_sec': encoding_time,
        'index_build_sec': index_build_time,
        'query_stats_ms': timing_stats
    },
    'system': {
        'corpus_embeddings_mb': corpus_embeddings_size_mb,
        'faiss_index_mb': faiss_index_size_mb,
        'total_memory_mb': corpus_embeddings_size_mb + faiss_index_size_mb
    },
    'config': {
        'bi_encoder': bi_encoder_config,
        'faiss': faiss_config
    },
    'comparison_with_bm25': {
        'bm25_ndcg10': bm25_ndcg10,
        'semantic_ndcg10': semantic_ndcg10,
        'improvement_percent': improvement
    }
}

with open(f"{PROJECT_DIR}/reports/semantic_retrieval_metrics.json", 'w') as f:
    json.dump(semantic_report, f, indent=2)

print(f"✅ Comprehensive report saved")

# =============================================================================
# SECTION 14: CLEANUP AND SUMMARY
# =============================================================================

# Memory cleanup
# del corpus_embeddings, all_results # Removed as variables are not in scope
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()

print("\n" + "="*60)
print("✅ SEMANTIC RETRIEVAL COMPLETED SUCCESSFULLY!")
print("="*60)

print(f"🎯 Semantic Retrieval Performance:")
print(f"   📊 nDCG@10: {semantic_metrics.get('nDCG@10', 0):.4f}")
print(f"   📊 nDCG@20: {semantic_metrics.get('nDCG@20', 0):.4f}")
print(f"   📊 MAP: {semantic_metrics.get('MAP', 0):.4f}")
print(f"   📊 MRR: {semantic_metrics.get('MRR', 0):.4f}")

print(f"\n🔄 BM25 vs Semantic Comparison:")
print(f"   📈 nDCG@10 Improvement: {improvement:+.1f}%")
print(f"   {'✅ Semantic Wins!' if semantic_ndcg10 > bm25_ndcg10 else '⚠️ Needs Tuning'}")

print(f"\n📊 System Performance:")
print(f"   🔍 Query Success Rate: {len(set([r['query_id'] for _, r in semantic_runs_df.iterrows()]))/len(queries_df)*100:.1f}%")
print(f"   ⏱️ p95 Latency: {timing_stats['p95_ms']:.2f} ms")
print(f"   💾 Total Memory: {semantic_report['system']['total_memory_mb']:.1f} MB")
print(f"   🚀 Throughput: {estimated_qps:.1f} QPS")

print(f"\n📁 Files Generated:")
print(f"   🔍 TREC Run: runs/bi_encoder_ann.trec")
print(f"   📊 Metrics: reports/semantic_retrieval_metrics.json")
print(f"   📈 Visualizations: reports/semantic_retrieval_analysis.png")
print(f"   🧠 Embeddings: indices/corpus_embeddings.npy")
print(f"   🔍 FAISS Index: indices/faiss_hnsw/")

print(f"\n🚀 Ready for Phase 4: Cross-Encoder Reranking!")
print(f"   Expected further improvements:")
print(f"   📈 nDCG@10: {semantic_metrics.get('nDCG@10', 0):.4f} → 0.10-0.25 (+100-300%)")
print(f"   🎯 Fine-grained relevance scoring")
print(f"   🔄 Reranking top-{top_k} semantic candidates")

print("="*60)

